{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# winddy\n",
    "\n",
    "用训练集CIFAR 对抗训练 ResNet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正常训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torchvision.transforms as transforms \n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from LeNet import LeNet\n",
    "import sys\n",
    "sys.path.append('./model')\n",
    "from resnet import ResNet18\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NORMALIZE = True\n",
    "RESUME = False\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str,[0,2,7]))\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NORMALIZE:\n",
    "    trans_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    trans_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "else:\n",
    "    trans_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    trans_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_home = '/data/winddy/'\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root=os.path.join(data_home, 'dataset/CIFAR10'), train=True, download=True, transform=trans_train)\n",
    "test_set = torchvision.datasets.CIFAR10(root=os.path.join(data_home, 'dataset/CIFAR10'), train=False, download=True, transform=trans_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 构建网络结构\n",
    "net = ResNet18()\n",
    "net = net.to(DEVICE)\n",
    "net = torch.nn.DataParallel(net)\n",
    "\n",
    "if RESUME:\n",
    "# Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        print('\\r batch_idx: {} | Loss: {} | Acc: {} '.format(batch_idx, \n",
    "                                                              train_loss/(batch_idx+1), 100.*correct/total ), end='')\n",
    "#         progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "#             % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            print('\\r batch_idx: {} | Loss: {} | Acc: {} '.format(batch_idx, \n",
    "                                                              test_loss/(batch_idx+1), 100.*correct/total ), end='')\n",
    "#             progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "#                 % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "            \n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.pth')\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      " batch_idx: 78 | Loss: 1.3250955675221696 | Acc: 56.83 4551282051 4  Saving..\n",
      "\n",
      "Epoch: 1\n",
      " batch_idx: 78 | Loss: 0.9298833668986454 | Acc: 67.66 2532051282    Saving..\n",
      "\n",
      "Epoch: 2\n",
      " batch_idx: 78 | Loss: 0.7305333297463912 | Acc: 75.27 44871794872  Saving..\n",
      "\n",
      "Epoch: 3\n",
      " batch_idx: 78 | Loss: 0.6912991412078278 | Acc: 77.11 4935897436   Saving..\n",
      "\n",
      "Epoch: 4\n",
      " batch_idx: 78 | Loss: 0.7177353218386445 | Acc: 77.05 23717948718  \n",
      "Epoch: 5\n",
      " batch_idx: 78 | Loss: 0.6310713445838494 | Acc: 79.27 84294871794   Saving..\n",
      "\n",
      "Epoch: 6\n",
      " batch_idx: 78 | Loss: 0.6136832286285449 | Acc: 79.28 79487179488   Saving..\n",
      "\n",
      "Epoch: 7\n",
      " batch_idx: 78 | Loss: 0.4970438027683693 | Acc: 83.99 440705128206  Saving..\n",
      "\n",
      "Epoch: 8\n",
      " batch_idx: 78 | Loss: 0.5072396141064318 | Acc: 83.77 2243589743    \n",
      "Epoch: 9\n",
      " batch_idx: 78 | Loss: 0.4806847044184238 | Acc: 84.26 79166666667   Saving..\n",
      "\n",
      "Epoch: 10\n",
      " batch_idx: 78 | Loss: 0.43260920575902434 | Acc: 85.68 673076923 1  Saving..\n",
      "\n",
      "Epoch: 11\n",
      " batch_idx: 78 | Loss: 0.5101283098323436 | Acc: 84.22 75961538461   \n",
      "Epoch: 12\n",
      " batch_idx: 78 | Loss: 0.406288493094565 | Acc: 86.61 854166666667   Saving..\n",
      "\n",
      "Epoch: 13\n",
      " batch_idx: 78 | Loss: 0.3697316725797291 | Acc: 87.84 052884615384  Saving..\n",
      "\n",
      "Epoch: 14\n",
      " batch_idx: 78 | Loss: 0.5244829386849946 | Acc: 83.45 2564102564    \n",
      "Epoch: 15\n",
      " batch_idx: 78 | Loss: 0.3697855187745034 | Acc: 88.2 110576923077   Saving..\n",
      "\n",
      "Epoch: 16\n",
      " batch_idx: 78 | Loss: 0.3945218249212337 | Acc: 87.4 983974358974   \n",
      "Epoch: 17\n",
      " batch_idx: 78 | Loss: 0.36213181441343284 | Acc: 88.39 45833333333  Saving..\n",
      "\n",
      "Epoch: 18\n",
      " batch_idx: 78 | Loss: 0.3603747256948978 | Acc: 88.51 60256410257   Saving..\n",
      "\n",
      "Epoch: 19\n",
      " batch_idx: 78 | Loss: 0.3318923847584785 | Acc: 89.27 283653846153  Saving..\n",
      "\n",
      "Epoch: 20\n",
      " batch_idx: 78 | Loss: 0.4269498001925553 | Acc: 86.8 86217948718 4  \n",
      "Epoch: 21\n",
      " batch_idx: 78 | Loss: 0.42743069103247 | Acc: 87.38 798076923077 6  \n",
      "Epoch: 22\n",
      " batch_idx: 78 | Loss: 0.33775108213288874 | Acc: 89.1 58012820512   \n",
      "Epoch: 23\n",
      " batch_idx: 78 | Loss: 0.3737664905529988 | Acc: 88.88 24358974359   \n",
      "Epoch: 24\n",
      " batch_idx: 78 | Loss: 0.3441489187976982 | Acc: 89.52 2532051282    Saving..\n",
      "\n",
      "Epoch: 25\n",
      " batch_idx: 78 | Loss: 0.3401206686526914 | Acc: 89.83 71794871794   Saving..\n",
      "\n",
      "Epoch: 26\n",
      " batch_idx: 78 | Loss: 0.335684341154521 | Acc: 89.53 326923076923   \n",
      "Epoch: 27\n",
      " batch_idx: 78 | Loss: 0.3537414660559425 | Acc: 89.22 77243589743   \n",
      "Epoch: 28\n",
      " batch_idx: 78 | Loss: 0.3330340968279899 | Acc: 90.22 375 5974025   Saving..\n",
      "\n",
      "Epoch: 29\n",
      " batch_idx: 78 | Loss: 0.3455312333152264 | Acc: 89.73 357371794872  \n",
      "Epoch: 30\n",
      " batch_idx: 78 | Loss: 0.3903346093772333 | Acc: 88.78 05128205128   \n",
      "Epoch: 31\n",
      " batch_idx: 78 | Loss: 0.34674013830438444 | Acc: 89.67 44551282051  \n",
      "Epoch: 32\n",
      " batch_idx: 78 | Loss: 0.33305821199960345 | Acc: 90.07 8653846153   \n",
      "Epoch: 33\n",
      " batch_idx: 78 | Loss: 0.35599046247669414 | Acc: 89.91 86217948718  \n",
      "Epoch: 34\n",
      " batch_idx: 78 | Loss: 0.34421711962057067 | Acc: 90.19 32692307692  \n",
      "Epoch: 35\n",
      " batch_idx: 78 | Loss: 0.30267151039612444 | Acc: 90.98 54487179488  Saving..\n",
      "\n",
      "Epoch: 36\n",
      " batch_idx: 78 | Loss: 0.3610235316843926 | Acc: 89.79 6858974359 1  \n",
      "Epoch: 37\n",
      " batch_idx: 78 | Loss: 0.3270750668229936 | Acc: 90.27 42307692308   \n",
      "Epoch: 38\n",
      " batch_idx: 78 | Loss: 0.3391414890183678 | Acc: 90.05 408653846153  \n",
      "Epoch: 39\n",
      " batch_idx: 78 | Loss: 0.3200796488908273 | Acc: 90.89 4871794872    \n",
      "Epoch: 40\n",
      " batch_idx: 78 | Loss: 0.3065948141149328 | Acc: 91.36 1858974359    Saving..\n",
      "\n",
      "Epoch: 41\n",
      " batch_idx: 78 | Loss: 0.3616828532724441 | Acc: 89.72 352564102564  \n",
      "Epoch: 42\n",
      " batch_idx: 78 | Loss: 0.3698177588514135 | Acc: 89.89 84615384616   \n",
      "Epoch: 43\n",
      " batch_idx: 78 | Loss: 0.31712213865941086 | Acc: 90.78 22435897436  \n",
      "Epoch: 44\n",
      " batch_idx: 78 | Loss: 0.3122827434275724 | Acc: 91.16 4935897436    \n",
      "Epoch: 45\n",
      " batch_idx: 78 | Loss: 0.3875516886952557 | Acc: 89.73 57371794872   \n",
      "Epoch: 46\n",
      " batch_idx: 78 | Loss: 0.3470143068440353 | Acc: 90.63 501602564102  \n",
      "Epoch: 47\n",
      " batch_idx: 78 | Loss: 0.3591212964133371 | Acc: 90.35 55128205128   \n",
      "Epoch: 48\n",
      " batch_idx: 78 | Loss: 0.3827508608751659 | Acc: 89.28 83653846153   \n",
      "Epoch: 49\n",
      " batch_idx: 78 | Loss: 0.31586049082158485 | Acc: 91.06 891025641    \n",
      "Epoch: 50\n",
      " batch_idx: 78 | Loss: 0.3617195436094381 | Acc: 90.11 19871794872   \n",
      "Epoch: 51\n",
      " batch_idx: 78 | Loss: 0.3462997876767871 | Acc: 90.54 83974358974   \n",
      "Epoch: 52\n",
      " batch_idx: 78 | Loss: 0.3368011267879341 | Acc: 90.6 5  921052632   \n",
      "Epoch: 53\n",
      " batch_idx: 78 | Loss: 0.36459480584422244 | Acc: 90.22 5897435898   \n",
      "Epoch: 54\n",
      " batch_idx: 78 | Loss: 0.35623897970477236 | Acc: 90.19 9487179488    \n",
      "Epoch: 55\n",
      " batch_idx: 78 | Loss: 0.30283431124083604 | Acc: 91.41 28205128206  Saving..\n",
      "\n",
      "Epoch: 56\n",
      " batch_idx: 78 | Loss: 0.32164829441263704 | Acc: 90.97 52884615384   \n",
      "Epoch: 57\n",
      " batch_idx: 78 | Loss: 0.3182883372978319 | Acc: 91.08 72115384616    \n",
      "Epoch: 58\n",
      " batch_idx: 78 | Loss: 0.3411131327665305 | Acc: 90.99 59294871794   \n",
      "Epoch: 59\n",
      " batch_idx: 78 | Loss: 0.3156093342017524 | Acc: 91.19 591346153847   \n",
      "Epoch: 60\n",
      " batch_idx: 78 | Loss: 0.30808111195322835 | Acc: 91.25 9358974359    \n",
      "Epoch: 61\n",
      " batch_idx: 78 | Loss: 0.28788708744547037 | Acc: 91.69 826923077     Saving..\n",
      "\n",
      "Epoch: 62\n",
      " batch_idx: 78 | Loss: 0.3196361655298668 | Acc: 91.37 21794871794    \n",
      "Epoch: 63\n",
      " batch_idx: 78 | Loss: 0.3031245486079892 | Acc: 91.67 65064102564    \n",
      "Epoch: 64\n",
      " batch_idx: 78 | Loss: 0.3517963920212999 | Acc: 90.46 71153846153  8 \n",
      "Epoch: 65\n",
      " batch_idx: 78 | Loss: 0.30972002239166935 | Acc: 91.19 8141025641    \n",
      "Epoch: 66\n",
      " batch_idx: 78 | Loss: 0.33504895054841344 | Acc: 90.64 03205128206   \n",
      "Epoch: 67\n",
      " batch_idx: 78 | Loss: 0.3005817471237122 | Acc: 91.59 652243589743 9 \n",
      "Epoch: 68\n",
      " batch_idx: 78 | Loss: 0.33611812527421153 | Acc: 91.01 625 194805  6 \n",
      "Epoch: 69\n",
      " batch_idx: 78 | Loss: 0.30220853103489814 | Acc: 91.32 10576923077   \n",
      "Epoch: 70\n",
      " batch_idx: 78 | Loss: 0.3219017043143888 | Acc: 91.39 23397435898    \n",
      "Epoch: 71\n",
      " batch_idx: 78 | Loss: 0.2882852664665331 | Acc: 91.65 63461538461    \n",
      "Epoch: 72\n",
      " batch_idx: 78 | Loss: 0.3109791086444372 | Acc: 91.22 96153846153    \n",
      "Epoch: 73\n",
      " batch_idx: 78 | Loss: 0.300207855392106 | Acc: 91.72 0673076923077 7 Saving..\n",
      "\n",
      "Epoch: 74\n",
      " batch_idx: 78 | Loss: 0.31434857694408563 | Acc: 91.27 4166666667  2 \n",
      "Epoch: 75\n",
      " batch_idx: 78 | Loss: 0.3373850372773183 | Acc: 90.36 458333333333  \n",
      "Epoch: 76\n",
      " batch_idx: 78 | Loss: 0.3532086301453506 | Acc: 90.44 471153846153   \n",
      "Epoch: 77\n",
      " batch_idx: 78 | Loss: 0.3272032701893698 | Acc: 90.84 32051282051    \n",
      "Epoch: 78\n",
      " batch_idx: 78 | Loss: 0.29356351579669154 | Acc: 91.67 66666666667   \n",
      "Epoch: 79\n",
      " batch_idx: 78 | Loss: 0.331115165654617 | Acc: 91.02 564102564102  7 \n",
      "Epoch: 80\n",
      " batch_idx: 78 | Loss: 0.2956527514925486 | Acc: 91.47 633012820512   \n",
      "Epoch: 81\n",
      " batch_idx: 78 | Loss: 0.29999289793681494 | Acc: 91.9 0673076923     Saving..\n",
      "\n",
      "Epoch: 82\n",
      " batch_idx: 78 | Loss: 0.2948899942485592 | Acc: 91.77 82692307692    \n",
      "Epoch: 83\n",
      " batch_idx: 78 | Loss: 0.2877703944716272 | Acc: 91.31 10576923077    \n",
      "Epoch: 84\n",
      " batch_idx: 78 | Loss: 0.32766734987874574 | Acc: 91.1 57532051282  1 \n",
      "Epoch: 85\n",
      " batch_idx: 78 | Loss: 0.31869368047653873 | Acc: 91.29 8974358974  4 \n",
      "Epoch: 86\n",
      " batch_idx: 78 | Loss: 0.3086588013021252 | Acc: 91.35 615384615384   \n",
      "Epoch: 87\n",
      " batch_idx: 78 | Loss: 0.2977210207830501 | Acc: 91.48 3782051282 1  \n",
      "Epoch: 88\n",
      " batch_idx: 78 | Loss: 0.2918467072746422 | Acc: 92.24 9615384616     Saving..\n",
      "\n",
      "Epoch: 89\n",
      " batch_idx: 78 | Loss: 0.3303489270089548 | Acc: 91.02 60897435898    \n",
      "Epoch: 90\n",
      " batch_idx: 78 | Loss: 0.3163356127429612 | Acc: 91.39 26602564102    \n",
      "Epoch: 91\n",
      " batch_idx: 78 | Loss: 0.2958293293095842 | Acc: 91.7 8669871794872   \n",
      "Epoch: 92\n",
      " batch_idx: 78 | Loss: 0.3002669019035146 | Acc: 91.56 9038461539     \n",
      "Epoch: 93\n",
      " batch_idx: 78 | Loss: 0.2857668664825114 | Acc: 91.97 14743589743    \n",
      "Epoch: 94\n",
      " batch_idx: 78 | Loss: 0.35903676618126373 | Acc: 90.51 076923077   4 \n",
      "Epoch: 95\n",
      " batch_idx: 78 | Loss: 0.3728601526987704 | Acc: 89.97 97435897436    \n",
      "Epoch: 96\n",
      " batch_idx: 78 | Loss: 0.29676397814403604 | Acc: 91.64 66666666667   \n",
      "Epoch: 97\n",
      " batch_idx: 78 | Loss: 0.35683825065063524 | Acc: 90.43 9551282051    \n",
      "Epoch: 98\n",
      " batch_idx: 78 | Loss: 0.30648936991450154 | Acc: 91.21 1346153847  7 \n",
      "Epoch: 99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_idx: 78 | Loss: 0.34959426663721666 | Acc: 90.63 98397435898   \n",
      "Epoch: 100\n",
      " batch_idx: 78 | Loss: 0.2864048461182208 | Acc: 91.88 698717948718   \n",
      "Epoch: 101\n",
      " batch_idx: 78 | Loss: 0.31656663817695424 | Acc: 91.24 96153846153   \n",
      "Epoch: 102\n",
      " batch_idx: 78 | Loss: 0.27775239152244374 | Acc: 92.36 5641025641    Saving..\n",
      "\n",
      "Epoch: 103\n",
      " batch_idx: 78 | Loss: 0.3557837994038304 | Acc: 90.36 45673076923    \n",
      "Epoch: 104\n",
      " batch_idx: 78 | Loss: 0.3111067242637465 | Acc: 91.55 650641025641 2 \n",
      "Epoch: 105\n",
      " batch_idx: 78 | Loss: 0.290947226595275 | Acc: 91.73 167467948718    \n",
      "Epoch: 106\n",
      " batch_idx: 78 | Loss: 0.3522158863046501 | Acc: 90.64 50641025641    \n",
      "Epoch: 107\n",
      " batch_idx: 78 | Loss: 0.3101107897826388 | Acc: 91.21 591346153847 6 \n",
      "Epoch: 108\n",
      " batch_idx: 78 | Loss: 0.30303065922064115 | Acc: 91.64 1858974359    \n",
      "Epoch: 109\n",
      " batch_idx: 78 | Loss: 0.3098806233345708 | Acc: 91.69 69871794872  2 \n",
      "Epoch: 110\n",
      " batch_idx: 78 | Loss: 0.32385309144288676 | Acc: 91.2 89743589743    \n",
      "Epoch: 111\n",
      " batch_idx: 78 | Loss: 0.3248743564833569 | Acc: 91.17 86538461539    \n",
      "Epoch: 112\n",
      " batch_idx: 78 | Loss: 0.33238053783963 | Acc: 90.82 533653846153 9   \n",
      "Epoch: 113\n",
      " batch_idx: 78 | Loss: 0.3099813280226309 | Acc: 91.4 23397435898     \n",
      "Epoch: 114\n",
      " batch_idx: 78 | Loss: 0.31508765011271345 | Acc: 91.51 42628205128   \n",
      "Epoch: 115\n",
      " batch_idx: 78 | Loss: 0.3125981571553629 | Acc: 91.14 173076923 11 2 \n",
      "Epoch: 116\n",
      " batch_idx: 78 | Loss: 0.36858911114402965 | Acc: 90.17 7884615384  4 \n",
      "Epoch: 117\n",
      " batch_idx: 78 | Loss: 0.3048511712422854 | Acc: 91.54 647435897436   \n",
      "Epoch: 118\n",
      " batch_idx: 78 | Loss: 0.27819358698929414 | Acc: 92.08 076923077     \n",
      "Epoch: 119\n",
      " batch_idx: 78 | Loss: 0.30187703725657883 | Acc: 91.42 26602564102   \n",
      "Epoch: 120\n",
      " batch_idx: 78 | Loss: 0.35939000150825406 | Acc: 90.29 5512820512  2 \n",
      "Epoch: 121\n",
      " batch_idx: 78 | Loss: 0.29465645419645914 | Acc: 91.94 9935897436  9 \n",
      "Epoch: 122\n",
      " batch_idx: 78 | Loss: 0.2812044834600219 | Acc: 92.25 61217948718    \n",
      "Epoch: 123\n",
      " batch_idx: 78 | Loss: 0.308505363951001 | Acc: 91.0 98557692307692 6 \n",
      "Epoch: 124\n",
      " batch_idx: 78 | Loss: 0.31420006433242487 | Acc: 90.83 33653846153 2 \n",
      "Epoch: 125\n",
      " batch_idx: 78 | Loss: 0.3318149330495279 | Acc: 91.0 560897435898    \n",
      "Epoch: 126\n",
      " batch_idx: 78 | Loss: 0.32448380959184864 | Acc: 91.22 6153846153    \n",
      "Epoch: 127\n",
      " batch_idx: 78 | Loss: 0.39414213428014444 | Acc: 89.72 576923077 4 2 \n",
      "Epoch: 128\n",
      " batch_idx: 78 | Loss: 0.2960668706063983 | Acc: 91.67 66666666667    \n",
      "Epoch: 129\n",
      " batch_idx: 78 | Loss: 0.29839582364015943 | Acc: 91.75 4294871794    \n",
      "Epoch: 130\n",
      " batch_idx: 78 | Loss: 0.32398460654518274 | Acc: 91.38 1858974359    \n",
      "Epoch: 131\n",
      " batch_idx: 78 | Loss: 0.31012833061852035 | Acc: 91.47 3012820512  1 \n",
      "Epoch: 132\n",
      " batch_idx: 78 | Loss: 0.3313680310038072 | Acc: 91.22 97756410257    \n",
      "Epoch: 133\n",
      " batch_idx: 78 | Loss: 0.3275728635018385 | Acc: 90.76 1923076923 4 6 \n",
      "Epoch: 134\n",
      " batch_idx: 78 | Loss: 0.3237440025881876 | Acc: 91.2 0592948717949   \n",
      "Epoch: 135\n",
      " batch_idx: 78 | Loss: 0.3101393030602721 | Acc: 91.61 655448717949   \n",
      "Epoch: 136\n",
      " batch_idx: 78 | Loss: 0.3001943507903739 | Acc: 91.97 16346153847    \n",
      "Epoch: 137\n",
      " batch_idx: 78 | Loss: 0.2921625033398218 | Acc: 91.96 11538461539    \n",
      "Epoch: 138\n",
      " batch_idx: 78 | Loss: 0.3100972923485539 | Acc: 91.66 671474358974   \n",
      "Epoch: 139\n",
      " batch_idx: 78 | Loss: 0.2976678911643692 | Acc: 92.3 1770833333333 7 \n",
      "Epoch: 140\n",
      " batch_idx: 78 | Loss: 0.31278025707866575 | Acc: 91.24 7756410257    \n",
      "Epoch: 141\n",
      " batch_idx: 78 | Loss: 0.3121014965485923 | Acc: 91.66 6826923077   2 \n",
      "Epoch: 142\n",
      " batch_idx: 78 | Loss: 0.31688217634830296 | Acc: 91.18 88141025641   \n",
      "Epoch: 143\n",
      " batch_idx: 78 | Loss: 0.2858828478787519 | Acc: 91.95 09935897436    \n",
      "Epoch: 144\n",
      " batch_idx: 78 | Loss: 0.27272010405984104 | Acc: 92.14 41987179488   \n",
      "Epoch: 145\n",
      " batch_idx: 78 | Loss: 0.29238052185200436 | Acc: 91.43 6602564102    \n",
      "Epoch: 146\n",
      " batch_idx: 78 | Loss: 0.31009386442130127 | Acc: 91.58 3846153847    \n",
      "Epoch: 147\n",
      " batch_idx: 78 | Loss: 0.2922276150010809 | Acc: 91.87 70032051282    \n",
      "Epoch: 148\n",
      " batch_idx: 78 | Loss: 0.31930756512322006 | Acc: 91.31 13782051282   \n",
      "Epoch: 149\n",
      " batch_idx: 78 | Loss: 0.3228785282448877 | Acc: 91.18 589743589743   \n",
      "Epoch: 150\n",
      " batch_idx: 78 | Loss: 0.31210360341245613 | Acc: 91.93 8333333333    \n",
      "Epoch: 151\n",
      " batch_idx: 78 | Loss: 0.317478134567979 | Acc: 91.18 6586538461539 2 \n",
      "Epoch: 152\n",
      " batch_idx: 78 | Loss: 0.28711508704891686 | Acc: 91.81 875 194805    \n",
      "Epoch: 153\n",
      " batch_idx: 78 | Loss: 0.29224421225393876 | Acc: 91.82 9102564102  1 \n",
      "Epoch: 154\n",
      " batch_idx: 78 | Loss: 0.31865400981299485 | Acc: 91.28 2564102564  8 \n",
      "Epoch: 155\n",
      " batch_idx: 78 | Loss: 0.267275986792166 | Acc: 92.14 740384615384  4 \n",
      "Epoch: 156\n",
      " batch_idx: 78 | Loss: 0.28659772891787033 | Acc: 91.67 66666666667   \n",
      "Epoch: 157\n",
      " batch_idx: 78 | Loss: 0.3313616385942773 | Acc: 91.13 580128205128 6 \n",
      "Epoch: 158\n",
      " batch_idx: 78 | Loss: 0.31059286168104483 | Acc: 91.4 625 79220779   \n",
      "Epoch: 159\n",
      " batch_idx: 78 | Loss: 0.3273134625787976 | Acc: 91.14 80128205128    \n",
      "Epoch: 160\n",
      " batch_idx: 78 | Loss: 0.26609030978966364 | Acc: 92.51 01282051282 2 Saving..\n",
      "\n",
      "Epoch: 161\n",
      " batch_idx: 78 | Loss: 0.3471837034331092 | Acc: 91.2 596153846153    \n",
      "Epoch: 162\n",
      " batch_idx: 78 | Loss: 0.3122950719693039 | Acc: 91.95 14743589743    \n",
      "Epoch: 163\n",
      " batch_idx: 78 | Loss: 0.2693429230889188 | Acc: 92.45 90064102564  9 \n",
      "Epoch: 164\n",
      " batch_idx: 78 | Loss: 0.3042150073790852 | Acc: 91.4 621794871794    \n",
      "Epoch: 165\n",
      " batch_idx: 78 | Loss: 0.27336104572573794 | Acc: 92.4 83653846153  8 \n",
      "Epoch: 166\n",
      " batch_idx: 78 | Loss: 0.3052779273896278 | Acc: 91.61 60256410257    \n",
      "Epoch: 167\n",
      " batch_idx: 78 | Loss: 0.31135350079098834 | Acc: 91.13 78525641026   \n",
      "Epoch: 168\n",
      " batch_idx: 78 | Loss: 0.303905462728271 | Acc: 91.63 5665064102564 4 \n",
      "Epoch: 169\n",
      " batch_idx: 78 | Loss: 0.30159751269259033 | Acc: 91.84 391025641   2 \n",
      "Epoch: 170\n",
      " batch_idx: 78 | Loss: 0.2877316072205954 | Acc: 91.97 3141025641     \n",
      "Epoch: 171\n",
      " batch_idx: 78 | Loss: 0.2820811555544032 | Acc: 92.32 70833333333  9 \n",
      "Epoch: 172\n",
      " batch_idx: 78 | Loss: 0.3358761555031885 | Acc: 91.16 88141025641    \n",
      "Epoch: 173\n",
      " batch_idx: 78 | Loss: 0.3019427609971807 | Acc: 91.91 70673076923    \n",
      "Epoch: 174\n",
      " batch_idx: 78 | Loss: 0.29897238898880873 | Acc: 91.75 8108974359  3 \n",
      "Epoch: 175\n",
      " batch_idx: 78 | Loss: 0.313422336797171 | Acc: 91.44 63141025641   4 \n",
      "Epoch: 176\n",
      " batch_idx: 78 | Loss: 0.26331822602431987 | Acc: 92.59 14102564102   Saving..\n",
      "\n",
      "Epoch: 177\n",
      " batch_idx: 78 | Loss: 0.300366339804251 | Acc: 91.48 636217948718  2 \n",
      "Epoch: 178\n",
      " batch_idx: 78 | Loss: 0.2922452250613442 | Acc: 91.49 36217948718    \n",
      "Epoch: 179\n",
      " batch_idx: 78 | Loss: 0.3403714370878437 | Acc: 91.26 60576923077    \n",
      "Epoch: 180\n",
      " batch_idx: 78 | Loss: 0.3586220136737522 | Acc: 90.77 524038461539 3 \n",
      "Epoch: 181\n",
      " batch_idx: 78 | Loss: 0.28101619795153415 | Acc: 92.44 90064102564   \n",
      "Epoch: 182\n",
      " batch_idx: 78 | Loss: 0.30720408770102486 | Acc: 91.62 653846153 8 6 \n",
      "Epoch: 183\n",
      " batch_idx: 78 | Loss: 0.31293545880272416 | Acc: 91.6 660256410257   \n",
      "Epoch: 184\n",
      " batch_idx: 78 | Loss: 0.3142267853210244 | Acc: 91.56 50641025641    \n",
      "Epoch: 185\n",
      " batch_idx: 78 | Loss: 0.2955496567714063 | Acc: 91.84 695512820512   \n",
      "Epoch: 186\n",
      " batch_idx: 78 | Loss: 0.37306380064427097 | Acc: 90.29 4391025641    \n",
      "Epoch: 187\n",
      " batch_idx: 78 | Loss: 0.3093664172706725 | Acc: 91.37 61858974359    \n",
      "Epoch: 188\n",
      " batch_idx: 78 | Loss: 0.28355138937506497 | Acc: 92.33 72435897436 2 \n",
      "Epoch: 189\n",
      " batch_idx: 78 | Loss: 0.35240053979656366 | Acc: 90.77 2435897436  3 \n",
      "Epoch: 190\n",
      " batch_idx: 78 | Loss: 0.3219198993866957 | Acc: 91.56 650641025641   \n",
      "Epoch: 191\n",
      " batch_idx: 78 | Loss: 0.28428087374077565 | Acc: 92.31 67628205128   \n",
      "Epoch: 192\n",
      " batch_idx: 78 | Loss: 0.27738551455962507 | Acc: 92.17  538961039  9 \n",
      "Epoch: 193\n",
      " batch_idx: 78 | Loss: 0.3470205057270919 | Acc: 90.67 08012820512  4 \n",
      "Epoch: 194\n",
      " batch_idx: 78 | Loss: 0.2992108263358285 | Acc: 91.91 0673076923   5 \n",
      "Epoch: 195\n",
      " batch_idx: 78 | Loss: 0.2988069664828385 | Acc: 91.98 717948717949   \n",
      "Epoch: 196\n",
      " batch_idx: 78 | Loss: 0.28543787100647067 | Acc: 91.99 6346153847    \n",
      "Epoch: 197\n",
      " batch_idx: 78 | Loss: 0.28829021648138387 | Acc: 92.03 22756410257   \n",
      "Epoch: 198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_idx: 78 | Loss: 0.32112033985838107 | Acc: 91.72 77884615384   \n",
      "Epoch: 199\n",
      " batch_idx: 78 | Loss: 0.302431238084277 | Acc: 91.71 9671474358974   "
     ]
    }
   ],
   "source": [
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0\n",
    "for epoch in range(start_epoch, start_epoch+200):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 保存模型\n",
    "if not os.path.exists('./model'):\n",
    "    os.makedirs('./model')\n",
    "if NORMALIZE:\n",
    "    model_path = './model/ResNet18_CIFAR10.pt'\n",
    "else:\n",
    "    model_path = './model/ResNet18_CIFAR10_unNormalize.pt'\n",
    "torch.save(net.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 对抗训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torchvision.transforms as transforms \n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from LeNet import LeNet\n",
    "import sys\n",
    "sys.path.append('./model')\n",
    "from resnet import ResNet18\n",
    "sys.path.append('./utils')\n",
    "from myUtils import my_fgsm, my_imshow\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NORMALIZE = True\n",
    "RESUME = True\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1,3,6,7\"\n",
    "DEVICE = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if NORMALIZE:\n",
    "    trans_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    trans_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "else:\n",
    "    trans_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    trans_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_home = '/data/winddy/'\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root=os.path.join(data_home, 'dataset/CIFAR10'), train=True, download=True, transform=trans_train)\n",
    "test_set = torchvision.datasets.CIFAR10(root=os.path.join(data_home, 'dataset/CIFAR10'), train=False, download=True, transform=trans_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=16)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=True, num_workers=16)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model: ./model/ResNet18_CIFAR10.pt\n"
     ]
    }
   ],
   "source": [
    "# 构建网络结构\n",
    "if NORMALIZE:\n",
    "    model_path = './model/ResNet18_CIFAR10.pt'\n",
    "else:\n",
    "    model_path = './model/ResNet18_CIFAR10_unNormalize.pt'\n",
    "\n",
    "model_adv = ResNet18()\n",
    "model_adv = model_adv.to(DEVICE)\n",
    "model_adv = torch.nn.DataParallel(model_adv, device_ids=[1,2,3])\n",
    "# , map_location=lambda storage, loc: storage.cuda(1)\n",
    "print('load model: {}'.format(model_path))\n",
    "model_adv.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# model_adv = model_adv.cpu()\n",
    "# model_adv = model_adv.to(DEVICE)\n",
    "# model_adv = torch.nn.DataParallel(model_adv)\n",
    "\n",
    "# # model_adv = torch.nn.DataParallel(model_adv)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_adv.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test... ...\n",
      " batch_idx: 78 | Loss: 0.3051270984018905 | Acc: 91.71 73076923077  "
     ]
    }
   ],
   "source": [
    "# 正常测试\n",
    "print('test... ...')\n",
    "\n",
    "model_adv.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        outputs = model_adv(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        print('\\r batch_idx: {} | Loss: {} | Acc: {} '.format(batch_idx, \n",
    "                                                          test_loss/(batch_idx+1), 100.*correct/total ), end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adversarial test... ...\n",
      " batch_idx: 78 | Loss: 4.841943517515931 | Acc: 23.99 8397435897434 "
     ]
    }
   ],
   "source": [
    "# 对砍测试\n",
    "print('adversarial test... ...')\n",
    "\n",
    "epsilon = 0.3\n",
    "model_adv.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "    inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "    inputs, sign = my_fgsm(inputs, targets, model_adv, criterion, epsilon, DEVICE)\n",
    "    outputs = model_adv(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    test_loss += loss.item()\n",
    "    _, predicted = outputs.max(1)\n",
    "    total += targets.size(0)\n",
    "    correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    print('\\r batch_idx: {} | Loss: {} | Acc: {} '.format(batch_idx, \n",
    "                                                      test_loss/(batch_idx+1), 100.*correct/total ), end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 0\n",
      " batch_idx: 390 | Loss: 1.7167409733128365 | Acc: 37.064 3397435897  test... ...\n",
      " batch_idx: 78 | Loss: 2.331100574022607 | Acc: 27.05 221153846153  \n",
      "epoch: 1\n",
      " batch_idx: 390 | Loss: 0.5275202078358902 | Acc: 81.726 076923077  test... ...\n",
      " batch_idx: 78 | Loss: 0.9699833234654197 | Acc: 68.97 32051282051 \n",
      "epoch: 2\n",
      " batch_idx: 390 | Loss: 0.374040286254395 | Acc: 87.544 20673076923  test... ...\n",
      " batch_idx: 78 | Loss: 0.7727149408074874 | Acc: 76.87 98076923077 \n",
      "epoch: 3\n",
      " batch_idx: 390 | Loss: 0.1937954072719035 | Acc: 93.738 97493573265 test... ...\n",
      " batch_idx: 78 | Loss: 0.5101497252530689 | Acc: 85.13 23397435898  \n",
      "epoch: 4\n",
      " batch_idx: 390 | Loss: 0.13921023524173384 | Acc: 95.562 1346153847 test... ...\n",
      " batch_idx: 78 | Loss: 0.5901166259110728 | Acc: 83.01 78846153847 \n",
      "epoch: 5\n",
      " batch_idx: 390 | Loss: 0.13996673890811098 | Acc: 95.418 7628205128 test... ...\n",
      " batch_idx: 78 | Loss: 0.6957542839684064 | Acc: 80.09 09294871794 \n",
      "epoch: 6\n",
      " batch_idx: 390 | Loss: 0.1010982888653074 | Acc: 96.794 86858974359 test... ...\n",
      " batch_idx: 78 | Loss: 0.5087258970435662 | Acc: 85.86 740384615384 \n",
      "epoch: 7\n",
      " batch_idx: 390 | Loss: 0.0938796471813908 | Acc: 97.062 29807692308 test... ...\n",
      " batch_idx: 78 | Loss: 0.6723640398888648 | Acc: 82.29 66666666667 \n",
      "epoch: 8\n",
      " batch_idx: 390 | Loss: 0.09053457624581464 | Acc: 97.086 3653846153 test... ...\n",
      " batch_idx: 78 | Loss: 1.060378173484078 | Acc: 74.04 844551282051 \n",
      "epoch: 9\n",
      " batch_idx: 390 | Loss: 0.0826679408607428 | Acc: 97.396 83974358974 test... ...\n",
      " batch_idx: 78 | Loss: 0.7575302595579172 | Acc: 80.33 54166666667 \n",
      "epoch: 10\n",
      " batch_idx: 390 | Loss: 0.07210759223555513 | Acc: 97.73 36858974359 test... ...\n",
      " batch_idx: 78 | Loss: 0.7501504692850234 | Acc: 81.06 74358974359 \n",
      "epoch: 11\n",
      " batch_idx: 390 | Loss: 0.07258574813699631 | Acc: 97.68 28205128206 test... ...\n",
      " batch_idx: 78 | Loss: 0.7711783798435067 | Acc: 80.7 913461538461 \n",
      "epoch: 12\n",
      " batch_idx: 390 | Loss: 0.06468111917357464 | Acc: 97.912 6025641026  test... ...\n",
      " batch_idx: 78 | Loss: 0.82661935347545 | Acc: 80.41 9863782051282 \n",
      "epoch: 13\n",
      " batch_idx: 390 | Loss: 0.0671420939495344 | Acc: 97.89 26282051282  test... ...\n",
      " batch_idx: 78 | Loss: 0.6929606055911584 | Acc: 82.11 39423076923 \n",
      "epoch: 14\n",
      " batch_idx: 390 | Loss: 0.06236362017100425 | Acc: 98.046 717948718   test... ...\n",
      " batch_idx: 78 | Loss: 0.9543030277083192 | Acc: 77.86 61538461539 \n",
      "epoch: 15\n",
      " batch_idx: 390 | Loss: 0.06894993774421379 | Acc: 97.838 4166666667 test... ...\n",
      " batch_idx: 78 | Loss: 0.9336090774475774 | Acc: 78.07 90384615384 \n",
      "epoch: 16\n",
      " batch_idx: 390 | Loss: 0.061457980290779375 | Acc: 98.072 346153847  test... ...\n",
      " batch_idx: 78 | Loss: 1.0501501416858239 | Acc: 75.45 6891025641  \n",
      "epoch: 17\n",
      " batch_idx: 390 | Loss: 0.0535355660387927 | Acc: 98.37 6738782051282 test... ...\n",
      " batch_idx: 78 | Loss: 0.8399516227124613 | Acc: 79.71 51602564102 \n",
      "epoch: 18\n",
      " batch_idx: 390 | Loss: 0.05883644170144482 | Acc: 98.148 3205128206  test... ...\n",
      " batch_idx: 78 | Loss: 0.9911989713016944 | Acc: 77.21 54166666667 \n",
      "epoch: 19\n",
      " batch_idx: 390 | Loss: 0.060913390982562626 | Acc: 98.056 8461538461 test... ...\n",
      " batch_idx: 78 | Loss: 0.9473814224895043 | Acc: 77.39 8141025641  \n",
      "epoch: 20\n",
      " batch_idx: 390 | Loss: 0.05386558243685671 | Acc: 98.36 38461538461  test... ...\n",
      " batch_idx: 78 | Loss: 1.2237197100361692 | Acc: 73.19 11538461539 \n",
      "epoch: 21\n",
      " batch_idx: 390 | Loss: 0.06388808348599602 | Acc: 98.006 108974359   test... ...\n",
      " batch_idx: 78 | Loss: 0.8855628420280505 | Acc: 78.6 58173076923  \n",
      "epoch: 22\n",
      " batch_idx: 390 | Loss: 0.05858363184954047 | Acc: 98.178 8333333333  test... ...\n",
      " batch_idx: 78 | Loss: 0.8586360782007628 | Acc: 78.28 20833333333 \n",
      "epoch: 23\n",
      " batch_idx: 390 | Loss: 0.05775339364567224 | Acc: 98.202 25 4832904  test... ...\n",
      " batch_idx: 78 | Loss: 0.811919938537139 | Acc: 80.43 865384615384 \n",
      "epoch: 24\n",
      " batch_idx: 390 | Loss: 0.05706663348752519 | Acc: 98.272 3717948718  test... ...\n",
      " batch_idx: 78 | Loss: 0.731845837982395 | Acc: 81.81 086538461539 \n",
      "epoch: 25\n",
      " batch_idx: 390 | Loss: 0.047773754795836974 | Acc: 98.524 423076923  test... ...\n",
      " batch_idx: 78 | Loss: 0.867709754011299 | Acc: 79.27 682692307692 \n",
      "epoch: 26\n",
      " batch_idx: 390 | Loss: 0.052443096557122364 | Acc: 98.386 666666667  test... ...\n",
      " batch_idx: 78 | Loss: 0.8262916018691244 | Acc: 80.18 28525641026 \n",
      "epoch: 27\n",
      " batch_idx: 390 | Loss: 0.05152234729484219 | Acc: 98.372 9102564102  test... ...\n",
      " batch_idx: 78 | Loss: 1.1146167961856988 | Acc: 74.56 27884615384 \n",
      "epoch: 28\n",
      " batch_idx: 390 | Loss: 0.06130814793355325 | Acc: 98.08 92307692308  test... ...\n",
      " batch_idx: 78 | Loss: 0.8984921710400642 | Acc: 78.24 22435897436 \n",
      "epoch: 29\n",
      " batch_idx: 390 | Loss: 0.05219677057774628 | Acc: 98.366 38782051282 test... ...\n",
      " batch_idx: 78 | Loss: 0.8588324521161332 | Acc: 79.7 759615384616 \n",
      "epoch: 30\n",
      " batch_idx: 390 | Loss: 0.057435581170003434 | Acc: 98.208 82051282 7 test... ...\n",
      " batch_idx: 78 | Loss: 0.8355753844297384 | Acc: 80.33 52564102564 \n",
      "epoch: 31\n",
      " batch_idx: 390 | Loss: 0.05446286994935302 | Acc: 98.37 39743589743  test... ...\n",
      " batch_idx: 78 | Loss: 0.7879251188869718 | Acc: 81.19 9358974359  \n",
      "epoch: 32\n",
      " batch_idx: 390 | Loss: 0.05559147413238845 | Acc: 98.248 9871794872  test... ...\n",
      " batch_idx: 78 | Loss: 0.8534833124921292 | Acc: 80.13 20512820512 \n",
      "epoch: 33\n",
      " batch_idx: 390 | Loss: 0.05531806365379592 | Acc: 98.288 5961538461  test... ...\n",
      " batch_idx: 78 | Loss: 0.856827344698242 | Acc: 79.61 73717948718  \n",
      "epoch: 34\n",
      " batch_idx: 390 | Loss: 0.055011748381511634 | Acc: 98.26 21474358974 test... ...\n",
      " batch_idx: 78 | Loss: 1.0532824291458613 | Acc: 76.49 38782051282 \n",
      "epoch: 35\n",
      " batch_idx: 390 | Loss: 0.06690593964189215 | Acc: 97.926 7628205128 test... ...\n",
      " batch_idx: 78 | Loss: 0.8662286151813555 | Acc: 79.6 3717948718 7 \n",
      "epoch: 36\n",
      " batch_idx: 390 | Loss: 0.053147884466878285 | Acc: 98.406 5192307692 test... ...\n",
      " batch_idx: 78 | Loss: 0.8089724398111995 | Acc: 80.69 10256410257 \n",
      "epoch: 37\n",
      " batch_idx: 390 | Loss: 0.05536607651949844 | Acc: 98.256 0833333333  test... ...\n",
      " batch_idx: 78 | Loss: 0.9361412502542327 | Acc: 78.44 48076923077 \n",
      "epoch: 38\n",
      " batch_idx: 390 | Loss: 0.05014956421921473 | Acc: 98.452 2564102564  test... ...\n",
      " batch_idx: 78 | Loss: 1.1002366376828543 | Acc: 75.61 96153846153 \n",
      "epoch: 39\n",
      " batch_idx: 390 | Loss: 0.058055556653177035 | Acc: 98.22 4743589743  test... ...\n",
      " batch_idx: 78 | Loss: 1.1254976731312425 | Acc: 74.35 97435897436 \n",
      "epoch: 40\n",
      " batch_idx: 390 | Loss: 0.05321049548285391 | Acc: 98.4 974358974359  test... ...\n",
      " batch_idx: 78 | Loss: 1.0438476748104337 | Acc: 76.62 6282051282  \n",
      "epoch: 41\n",
      " batch_idx: 390 | Loss: 0.05216519809459024 | Acc: 98.41 45512820512  test... ...\n",
      " batch_idx: 78 | Loss: 1.3564565415623822 | Acc: 70.26 40384615384 \n",
      "epoch: 42\n",
      " batch_idx: 390 | Loss: 0.053122489891774816 | Acc: 98.32 076923077   test... ...\n",
      " batch_idx: 78 | Loss: 1.057658825494066 | Acc: 76.08 7467948718 9 \n",
      "epoch: 43\n",
      " batch_idx: 390 | Loss: 0.0581135971571707 | Acc: 98.27 722756410257  test... ...\n",
      " batch_idx: 78 | Loss: 0.7974102233029619 | Acc: 80.73 16666666667 \n",
      "epoch: 44\n",
      " batch_idx: 390 | Loss: 0.05654481887017065 | Acc: 98.224 6025641026  test... ...\n",
      " batch_idx: 78 | Loss: 0.9942203801644 | Acc: 77.14 11338141025641 \n",
      "epoch: 45\n",
      " batch_idx: 390 | Loss: 0.058321390043744044 | Acc: 98.212 4423076923 test... ...\n",
      " batch_idx: 78 | Loss: 0.9260784002799022 | Acc: 78.16 04807692308 \n",
      "epoch: 46\n",
      " batch_idx: 390 | Loss: 0.05797226097949249 | Acc: 98.19 09935897436  test... ...\n",
      " batch_idx: 78 | Loss: 1.2496805440021466 | Acc: 72.25 59294871794 \n",
      "epoch: 47\n",
      " batch_idx: 390 | Loss: 0.056597330767061096 | Acc: 98.256 153846153  test... ...\n",
      " batch_idx: 78 | Loss: 0.8066297679007808 | Acc: 81.08 72756410257 \n",
      "epoch: 48\n",
      " batch_idx: 390 | Loss: 0.05313877224960291 | Acc: 98.348 5576923077  test... ...\n",
      " batch_idx: 78 | Loss: 1.189340049707437 | Acc: 73.37 740384615384 \n",
      "epoch: 49\n",
      " batch_idx: 390 | Loss: 0.05438865092404358 | Acc: 98.288 82051282    test... ...\n",
      " batch_idx: 78 | Loss: 1.0246379051027419 | Acc: 76.78 86858974359 \n",
      "epoch: 50\n",
      " batch_idx: 390 | Loss: 0.05584652271702924 | Acc: 98.318 08974359 5  test... ...\n",
      " batch_idx: 78 | Loss: 0.7509171955193146 | Acc: 81.87 96153846153 \n",
      "epoch: 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_idx: 390 | Loss: 0.04799989977722888 | Acc: 98.462 54487179488 test... ...\n",
      " batch_idx: 78 | Loss: 1.0717971151388144 | Acc: 75.85 36217948718 \n",
      "epoch: 52\n",
      " batch_idx: 390 | Loss: 0.054578789088236705 | Acc: 98.272 397435898  test... ...\n",
      " batch_idx: 78 | Loss: 0.9879989684382572 | Acc: 77.8 451923076923 \n",
      "epoch: 53\n",
      " batch_idx: 390 | Loss: 0.05585531074353649 | Acc: 98.234 7628205128  test... ...\n",
      " batch_idx: 78 | Loss: 1.016937546337707 | Acc: 76.29 203525641026 \n",
      "epoch: 54\n",
      " batch_idx: 390 | Loss: 0.051310732941645794 | Acc: 98.386 666666667  test... ...\n",
      " batch_idx: 78 | Loss: 0.8721895353703559 | Acc: 79.87 80448717949 \n",
      "epoch: 55\n",
      " batch_idx: 390 | Loss: 0.05457351820262344 | Acc: 98.332 3012820512  test... ...\n",
      " batch_idx: 78 | Loss: 0.9581662899331201 | Acc: 77.67 6282051282  \n",
      "epoch: 56\n",
      " batch_idx: 390 | Loss: 0.05095545358746253 | Acc: 98.448 52243589743 test... ...\n",
      " batch_idx: 78 | Loss: 0.9418792234191412 | Acc: 78.41 44871794872 \n",
      "epoch: 57\n",
      " batch_idx: 390 | Loss: 0.04536803896584169 | Acc: 98.638 2692307692  test... ...\n",
      " batch_idx: 78 | Loss: 1.1213444192198259 | Acc: 74.78 64743589743 \n",
      "epoch: 58\n",
      " batch_idx: 390 | Loss: 0.0543277164554352 | Acc: 98.3 29727564102564 test... ...\n",
      " batch_idx: 78 | Loss: 0.9236283011828796 | Acc: 79.47 9551282051  \n",
      "epoch: 59\n",
      " batch_idx: 390 | Loss: 0.05541889839674658 | Acc: 98.33 33012820512  test... ...\n",
      " batch_idx: 78 | Loss: 0.9885516393033764 | Acc: 76.75 82051282051 \n",
      "epoch: 60\n",
      " batch_idx: 390 | Loss: 0.05679227328380508 | Acc: 98.224 6025641026  test... ...\n",
      " batch_idx: 78 | Loss: 0.8101858398582362 | Acc: 80.54 83012820512 \n",
      "epoch: 61\n",
      " batch_idx: 390 | Loss: 0.04755123973826466 | Acc: 98.566 70512820512 test... ...\n",
      " batch_idx: 78 | Loss: 0.9110735760459417 | Acc: 78.43 4967948718  \n",
      "epoch: 62\n",
      " batch_idx: 390 | Loss: 0.05364619773786391 | Acc: 98.32 3076923077 3 test... ...\n",
      " batch_idx: 78 | Loss: 1.2556301717516742 | Acc: 72.41 6538461539  \n",
      "epoch: 63\n",
      " batch_idx: 390 | Loss: 0.05878200061866999 | Acc: 98.178 8333333333  test... ...\n",
      " batch_idx: 78 | Loss: 0.8533843959434123 | Acc: 78.98 3782051282  \n",
      "epoch: 64\n",
      " batch_idx: 390 | Loss: 0.05319196422634375 | Acc: 98.406 5512820512  test... ...\n",
      " batch_idx: 78 | Loss: 0.9618969310688067 | Acc: 77.38 8205128206  \n",
      "epoch: 65\n",
      " batch_idx: 390 | Loss: 0.05144685362001209 | Acc: 98.428 8717948718  test... ...\n",
      " batch_idx: 78 | Loss: 0.7229618399958068 | Acc: 82.48 95512820512 \n",
      "epoch: 66\n",
      " batch_idx: 390 | Loss: 0.05273375394837478 | Acc: 98.386 1666666667  test... ...\n",
      " batch_idx: 78 | Loss: 0.7639971020855482 | Acc: 81.2 990384615384 \n",
      "epoch: 67\n",
      " batch_idx: 390 | Loss: 0.05033467387032631 | Acc: 98.422 756410257   test... ...\n",
      " batch_idx: 78 | Loss: 0.711392790833606 | Acc: 82.52 203525641026 \n",
      "epoch: 68\n",
      " batch_idx: 390 | Loss: 0.051434086040234014 | Acc: 98.406 4871794872 test... ...\n",
      " batch_idx: 78 | Loss: 0.8865879977805705 | Acc: 79.79 66025641026 \n",
      "epoch: 69\n",
      " batch_idx: 390 | Loss: 0.06069381986184955 | Acc: 98.15 04807692308  test... ...\n",
      " batch_idx: 78 | Loss: 0.9504904015154778 | Acc: 76.98 14102564102 \n",
      "epoch: 70\n",
      " batch_idx: 390 | Loss: 0.053994298105120964 | Acc: 98.316 448717949  test... ...\n",
      " batch_idx: 78 | Loss: 0.7962017979802964 | Acc: 80.56 025641026 3 \n",
      "epoch: 71\n",
      " batch_idx: 390 | Loss: 0.04899352552164394 | Acc: 98.484 8333333333  test... ...\n",
      " batch_idx: 78 | Loss: 1.0257269076154203 | Acc: 77.23 57371794872 \n",
      "epoch: 72\n",
      " batch_idx: 390 | Loss: 0.049054825559372794 | Acc: 98.52 63141025641 test... ...\n",
      " batch_idx: 78 | Loss: 1.0704899143569078 | Acc: 76.25 03525641026 \n",
      "epoch: 73\n",
      " batch_idx: 390 | Loss: 0.04877770226210584 | Acc: 98.434 4967948718  test... ...\n",
      " batch_idx: 78 | Loss: 0.9700282710262492 | Acc: 77.23 5576923077  \n",
      "epoch: 74\n",
      " batch_idx: 390 | Loss: 0.05609155206195534 | Acc: 98.248 923076923 6 test... ...\n",
      " batch_idx: 78 | Loss: 0.8702141812330559 | Acc: 79.26 77884615384 \n",
      "epoch: 75\n",
      " batch_idx: 390 | Loss: 0.051801113521351534 | Acc: 98.374 423076923  test... ...\n",
      " batch_idx: 78 | Loss: 0.9310257457479646 | Acc: 78.18 03205128206 \n",
      "epoch: 76\n",
      " batch_idx: 390 | Loss: 0.05639135468836941 | Acc: 98.262 21794871794 test... ...\n",
      " batch_idx: 78 | Loss: 0.9116259560554842 | Acc: 78.76 04166666667 \n",
      "epoch: 77\n",
      " batch_idx: 390 | Loss: 0.05606961398459304 | Acc: 98.262 435897436   test... ...\n",
      " batch_idx: 78 | Loss: 0.6785820726352402 | Acc: 82.52 98717948718 \n",
      "epoch: 78\n",
      " batch_idx: 390 | Loss: 0.04675767364937936 | Acc: 98.514 1858974359  test... ...\n",
      " batch_idx: 78 | Loss: 0.7144712085210825 | Acc: 82.84 51602564102 \n",
      "epoch: 79\n",
      " batch_idx: 390 | Loss: 0.05096516482379583 | Acc: 98.476 5673076923  test... ...\n",
      " batch_idx: 78 | Loss: 0.7930183704895309 | Acc: 81.05 67948717949 \n",
      "epoch: 80\n",
      " batch_idx: 390 | Loss: 0.050104653072136136 | Acc: 98.472 5128205128 test... ...\n",
      " batch_idx: 78 | Loss: 0.7707091083254995 | Acc: 81.48 35256410257 \n",
      "epoch: 81\n",
      " batch_idx: 390 | Loss: 0.05852291588683415 | Acc: 98.234 7307692308  test... ...\n",
      " batch_idx: 78 | Loss: 0.70370002260691 | Acc: 82.92 6274038461539 \n",
      "epoch: 82\n",
      " batch_idx: 390 | Loss: 0.06275089869223287 | Acc: 98.024 3974358974  test... ...\n",
      " batch_idx: 78 | Loss: 0.7005207949801336 | Acc: 82.21 55448717949 \n",
      "epoch: 83\n",
      " batch_idx: 390 | Loss: 0.05753381272582599 | Acc: 98.206 125 40617   test... ...\n",
      " batch_idx: 78 | Loss: 0.7884204796975172 | Acc: 80.36 5576923077  \n",
      "epoch: 84\n",
      " batch_idx: 390 | Loss: 0.05692627805921123 | Acc: 98.234 891025641   test... ...\n",
      " batch_idx: 78 | Loss: 0.5845279067377501 | Acc: 85.25 3782051282  \n",
      "epoch: 85\n",
      " batch_idx: 390 | Loss: 0.07253387857161825 | Acc: 97.664 6282051282 test... ...\n",
      " batch_idx: 78 | Loss: 0.79474799014345 | Acc: 80.54 3886217948718 \n",
      "epoch: 86\n",
      " batch_idx: 390 | Loss: 0.05684601557929345 | Acc: 98.194 11538461539 test... ...\n",
      " batch_idx: 78 | Loss: 0.8721358798727205 | Acc: 79.26 82692307692 \n",
      "epoch: 87\n",
      " batch_idx: 390 | Loss: 0.05798431190059466 | Acc: 98.204 25 84832904 test... ...\n",
      " batch_idx: 78 | Loss: 0.619516675230823 | Acc: 85.06 610576923077 \n",
      "epoch: 88\n",
      " batch_idx: 390 | Loss: 0.06377584112765235 | Acc: 98.094 4871794872  test... ...\n",
      " batch_idx: 78 | Loss: 0.7988032596775249 | Acc: 80.59 95833333333 \n",
      "epoch: 89\n",
      " batch_idx: 390 | Loss: 0.051293878375416824 | Acc: 98.43 9038461539  test... ...\n",
      " batch_idx: 78 | Loss: 0.7485124329222909 | Acc: 81.58 54487179488 \n",
      "epoch: 90\n",
      " batch_idx: 390 | Loss: 0.04895722619293596 | Acc: 98.508 1538461539  test... ...\n",
      " batch_idx: 78 | Loss: 1.0514665283734286 | Acc: 76.55 5 61038961  \n",
      "epoch: 91\n",
      " batch_idx: 390 | Loss: 0.04798228456102826 | Acc: 98.574 72756410257 test... ...\n",
      " batch_idx: 78 | Loss: 0.6635985774329946 | Acc: 83.71 95833333333 \n",
      "epoch: 92\n",
      " batch_idx: 390 | Loss: 0.05008833614342353 | Acc: 98.438   6185567   test... ...\n",
      " batch_idx: 78 | Loss: 0.6920240313946446 | Acc: 83.5 358974358974 \n",
      "epoch: 93\n",
      " batch_idx: 390 | Loss: 0.05310601087482384 | Acc: 98.408 6474358974  test... ...\n",
      " batch_idx: 78 | Loss: 0.764377346898936 | Acc: 81.89 102564102564 \n",
      "epoch: 94\n",
      " batch_idx: 390 | Loss: 0.05322521843035203 | Acc: 98.364 8141025641  test... ...\n",
      " batch_idx: 78 | Loss: 0.6745978027959413 | Acc: 83.43 50961538461 \n",
      "epoch: 95\n",
      " batch_idx: 390 | Loss: 0.05929650417755327 | Acc: 98.196 1858974359  test... ...\n",
      " batch_idx: 78 | Loss: 0.5036845348681076 | Acc: 86.85 9423076923   \n",
      "epoch: 96\n",
      " batch_idx: 390 | Loss: 0.055929010519591135 | Acc: 98.294 64102564   test... ...\n",
      " batch_idx: 78 | Loss: 0.9868690265884882 | Acc: 77.47 91025641026 \n",
      "epoch: 97\n",
      " batch_idx: 390 | Loss: 0.0523254886279097 | Acc: 98.394 342948717949 test... ...\n",
      " batch_idx: 78 | Loss: 0.7568837562693825 | Acc: 82.24 58653846153 \n",
      "epoch: 98\n",
      " batch_idx: 390 | Loss: 0.05039279609251662 | Acc: 98.446 51282051282 test... ...\n",
      " batch_idx: 78 | Loss: 0.7466302291501926 | Acc: 81.98 6987179488  \n",
      "epoch: 99\n",
      " batch_idx: 390 | Loss: 0.05263290485209974 | Acc: 98.358 82051282 2  test... ...\n",
      " batch_idx: 78 | Loss: 0.6254763195786295 | Acc: 83.93 27884615384 \n",
      "epoch: 100\n",
      " batch_idx: 390 | Loss: 0.055442323012615716 | Acc: 98.244 8974359 5  test... ...\n",
      " batch_idx: 78 | Loss: 0.4850484730699394 | Acc: 86.65 860576923077 \n",
      "epoch: 101\n",
      " batch_idx: 390 | Loss: 0.05699392891658084 | Acc: 98.23 17628205128  test... ...\n",
      " batch_idx: 78 | Loss: 0.6519103744361974 | Acc: 84.13 7948717949  \n",
      "epoch: 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_idx: 390 | Loss: 0.04996374842074826 | Acc: 98.53 64743589743  test... ...\n",
      " batch_idx: 78 | Loss: 0.8909506141384945 | Acc: 80.26 46153846153 \n",
      "epoch: 103\n",
      " batch_idx: 390 | Loss: 0.05735547697204915 | Acc: 98.204 3461538461  test... ...\n",
      " batch_idx: 78 | Loss: 0.5151768786997735 | Acc: 86.82 91025641026 \n",
      "epoch: 104\n",
      " batch_idx: 390 | Loss: 0.05556590238686108 | Acc: 98.252 92307692 8  test... ...\n",
      " batch_idx: 78 | Loss: 0.6825889058505432 | Acc: 82.77 40384615384 \n",
      "epoch: 105\n",
      " batch_idx: 390 | Loss: 0.04553359056182225 | Acc: 98.554 858974359 9 test... ...\n",
      " batch_idx: 78 | Loss: 0.6240606504150584 | Acc: 84.37 96794871794 \n",
      "epoch: 106\n",
      " batch_idx: 390 | Loss: 0.04912189004556907 | Acc: 98.396 4358974359  test... ...\n",
      " batch_idx: 78 | Loss: 0.6973944068709507 | Acc: 83.03 83653846153 \n",
      "epoch: 107\n",
      " batch_idx: 390 | Loss: 0.04629948814792554 | Acc: 98.614 8525641026  test... ...\n",
      " batch_idx: 78 | Loss: 0.816856311846383 | Acc: 80.56 88782051282  \n",
      "epoch: 108\n",
      " batch_idx: 390 | Loss: 0.04009218556601602 | Acc: 98.766 02243589743 test... ...\n",
      " batch_idx: 78 | Loss: 0.6375117879125136 | Acc: 84.44 11217948718 \n",
      "epoch: 109\n",
      " batch_idx: 390 | Loss: 0.04819380021308694 | Acc: 98.516 282051282   test... ...\n",
      " batch_idx: 78 | Loss: 0.7575065263464481 | Acc: 82.42 75 4415585  \n",
      "epoch: 110\n",
      " batch_idx: 390 | Loss: 0.04713050026894378 | Acc: 98.586 3717948718  test... ...\n",
      " batch_idx: 78 | Loss: 0.7825857766821415 | Acc: 81.37 20833333333 \n",
      "epoch: 111\n",
      " batch_idx: 390 | Loss: 0.05036269801447306 | Acc: 98.466 4487179488  test... ...\n",
      " batch_idx: 78 | Loss: 0.6857243715962277 | Acc: 82.79 45192307692 \n",
      "epoch: 112\n",
      " batch_idx: 390 | Loss: 0.046355370165365735 | Acc: 98.618 846153847  test... ...\n",
      " batch_idx: 78 | Loss: 0.8280140206783633 | Acc: 81.2 995192307692 \n",
      "epoch: 113\n",
      " batch_idx: 390 | Loss: 0.051667412547656645 | Acc: 98.392 307692308  test... ...\n",
      " batch_idx: 78 | Loss: 1.1224597345424603 | Acc: 75.46 76923076923 \n",
      "epoch: 114\n",
      " batch_idx: 390 | Loss: 0.04458277551528743 | Acc: 98.62 79487179488  test... ...\n",
      " batch_idx: 78 | Loss: 0.9190609470198426 | Acc: 79.46 14743589743 \n",
      "epoch: 115\n",
      " batch_idx: 390 | Loss: 0.049125921009751536 | Acc: 98.436 67948718 1 test... ...\n",
      " batch_idx: 78 | Loss: 0.852545395304885 | Acc: 80.45 870192307692 \n",
      "epoch: 116\n",
      " batch_idx: 390 | Loss: 0.04610669122213293 | Acc: 98.604 6602564102  test... ...\n",
      " batch_idx: 78 | Loss: 0.7659506918508795 | Acc: 82.04 23397435898 \n",
      "epoch: 117\n",
      " batch_idx: 390 | Loss: 0.05444054600432553 | Acc: 98.27 23076923077  test... ...\n",
      " batch_idx: 78 | Loss: 0.6902157215377952 | Acc: 82.2 158653846153 \n",
      "epoch: 118\n",
      " batch_idx: 390 | Loss: 0.05239238039783352 | Acc: 98.372 423076923   test... ...\n",
      " batch_idx: 78 | Loss: 0.8399385534509828 | Acc: 80.52 86217948718 \n",
      "epoch: 119\n",
      " batch_idx: 390 | Loss: 0.05444676183697665 | Acc: 98.346 35256410257 test... ...\n",
      " batch_idx: 78 | Loss: 0.7764092438583132 | Acc: 81.58 51282051282 \n",
      "epoch: 120\n",
      " batch_idx: 390 | Loss: 0.04791213615375864 | Acc: 98.54 66346153847  test... ...\n",
      " batch_idx: 78 | Loss: 0.715478033959111 | Acc: 82.96 74038461539  \n",
      "epoch: 121\n",
      " batch_idx: 390 | Loss: 0.047631178470447545 | Acc: 98.584 97435898   test... ...\n",
      " batch_idx: 78 | Loss: 0.5138519395001327 | Acc: 87.06 3108974359   \n",
      "epoch: 122\n",
      " batch_idx: 390 | Loss: 0.04572654051510879 | Acc: 98.656 85256410257 test... ...\n",
      " batch_idx: 78 | Loss: 0.6731063636797893 | Acc: 83.49 58974358974 \n",
      "epoch: 123\n",
      " batch_idx: 390 | Loss: 0.04665503343638709 | Acc: 98.618 9487179488  test... ...\n",
      " batch_idx: 78 | Loss: 0.5778071069264714 | Acc: 86.09 74038461539 \n",
      "epoch: 124\n",
      " batch_idx: 390 | Loss: 0.04260550598945002 | Acc: 98.712 423076923   test... ...\n",
      " batch_idx: 78 | Loss: 0.6249476761757573 | Acc: 84.92 4935897436  \n",
      "epoch: 125\n",
      " batch_idx: 390 | Loss: 0.05302128567815284 | Acc: 98.338 4935897436  test... ...\n",
      " batch_idx: 78 | Loss: 0.5460365501385701 | Acc: 86.57 50961538461 \n",
      "epoch: 126\n",
      " batch_idx: 390 | Loss: 0.05207766366222173 | Acc: 98.422 7435897436  test... ...\n",
      " batch_idx: 78 | Loss: 0.6914196823593937 | Acc: 82.47 97115384616 \n",
      "epoch: 127\n",
      " batch_idx: 390 | Loss: 0.05807886932454908 | Acc: 98.256 0512820512  test... ...\n",
      " batch_idx: 78 | Loss: 0.6489540515821192 | Acc: 83.26 23717948718 \n",
      "epoch: 128\n",
      " batch_idx: 390 | Loss: 0.046030437359419626 | Acc: 98.586 3717948718 test... ...\n",
      " batch_idx: 78 | Loss: 0.7042104194435892 | Acc: 82.88 58012820512 \n",
      "epoch: 129\n",
      " batch_idx: 390 | Loss: 0.05262022479758848 | Acc: 98.402 4423076923  test... ...\n",
      " batch_idx: 78 | Loss: 0.9015437330626235 | Acc: 79.18 5064102564  \n",
      "epoch: 130\n",
      " batch_idx: 390 | Loss: 0.040696385564268245 | Acc: 98.796 051282051  test... ...\n",
      " batch_idx: 78 | Loss: 1.057146170471288 | Acc: 76.84 90064102564  \n",
      "epoch: 131\n",
      " batch_idx: 390 | Loss: 0.051279338838918435 | Acc: 98.406 4551282051 test... ...\n",
      " batch_idx: 78 | Loss: 0.7726899317548245 | Acc: 81.24 98397435898 \n",
      "epoch: 132\n",
      " batch_idx: 390 | Loss: 0.04831285639415921 | Acc: 98.57 71474358974  test... ...\n",
      " batch_idx: 78 | Loss: 0.976154595236235 | Acc: 78.94 628205128206 \n",
      "epoch: 133\n",
      " batch_idx: 390 | Loss: 0.05040952941055036 | Acc: 98.494 59615384616 test... ...\n",
      " batch_idx: 78 | Loss: 0.8200363890279697 | Acc: 80.76 23076923077 \n",
      "epoch: 134\n",
      " batch_idx: 390 | Loss: 0.0494178817524096 | Acc: 98.514 1858974359   test... ...\n",
      " batch_idx: 78 | Loss: 0.9778366036052946 | Acc: 77.56 07051282051 \n",
      "epoch: 135\n",
      " batch_idx: 390 | Loss: 0.04431365485138753 | Acc: 98.626 80128205128 test... ...\n",
      " batch_idx: 78 | Loss: 0.6433336606508568 | Acc: 84.71 1282051282  \n",
      "epoch: 136\n",
      " batch_idx: 390 | Loss: 0.04150897304496497 | Acc: 98.784 576923077 4 test... ...\n",
      " batch_idx: 78 | Loss: 0.75095958347562 | Acc: 82.35 7179487179488 \n",
      "epoch: 137\n",
      " batch_idx: 390 | Loss: 0.051466205474132165 | Acc: 98.434 967948718  test... ...\n",
      " batch_idx: 78 | Loss: 0.9202464539793473 | Acc: 78.35 3653846153  \n",
      "epoch: 138\n",
      " batch_idx: 390 | Loss: 0.0467355923341287 | Acc: 98.608 77243589743  test... ...\n",
      " batch_idx: 78 | Loss: 0.9022510813761361 | Acc: 78.99 782051282   \n",
      "epoch: 139\n",
      " batch_idx: 390 | Loss: 0.049690661122998615 | Acc: 98.532 384615384  test... ...\n",
      " batch_idx: 78 | Loss: 0.8968019130863721 | Acc: 79.4 70673076923  \n",
      "epoch: 140\n",
      " batch_idx: 390 | Loss: 0.047416921847921505 | Acc: 98.568 833333333  test... ...\n",
      " batch_idx: 78 | Loss: 1.152818624354616 | Acc: 74.76 959935897436 \n",
      "epoch: 141\n",
      " batch_idx: 390 | Loss: 0.05174572198935177 | Acc: 98.426 8076923077  test... ...\n",
      " batch_idx: 78 | Loss: 0.6247479247895977 | Acc: 84.2 471153846153 \n",
      "epoch: 142\n",
      " batch_idx: 390 | Loss: 0.047560617461076475 | Acc: 98.564 0833333333 test... ...\n",
      " batch_idx: 78 | Loss: 0.8687140111681781 | Acc: 79.99 01282051282 \n",
      "epoch: 143\n",
      " batch_idx: 390 | Loss: 0.046811806052313436 | Acc: 98.638 051282051  test... ...\n",
      " batch_idx: 78 | Loss: 0.6567415742180015 | Acc: 83.42 49358974359 \n",
      "epoch: 144\n",
      " batch_idx: 390 | Loss: 0.048228336893536554 | Acc: 98.524 461538461  test... ...\n",
      " batch_idx: 78 | Loss: 0.5271127827559845 | Acc: 86.62 58974358974 \n",
      "epoch: 145\n",
      " batch_idx: 390 | Loss: 0.048438097981502636 | Acc: 98.494 8974358974 test... ...\n",
      " batch_idx: 78 | Loss: 1.0501776420617406 | Acc: 76.13 8108974359  \n",
      "epoch: 146\n",
      " batch_idx: 390 | Loss: 0.04623330573139288 | Acc: 98.628 108974359 5 test... ...\n",
      " batch_idx: 78 | Loss: 0.8822591523580914 | Acc: 80.11 15705128206 \n",
      "epoch: 147\n",
      " batch_idx: 390 | Loss: 0.04793333614249821 | Acc: 98.582 73397435898 test... ...\n",
      " batch_idx: 78 | Loss: 0.7456300945221623 | Acc: 82.03 23397435898 \n",
      "epoch: 148\n",
      " batch_idx: 390 | Loss: 0.039875364531298425 | Acc: 98.834 782051282  test... ...\n",
      " batch_idx: 78 | Loss: 0.7299401722376859 | Acc: 83.88 23076923077 \n",
      "epoch: 149\n",
      " batch_idx: 390 | Loss: 0.05276646774233607 | Acc: 98.376 0384615384  test... ...\n",
      " batch_idx: 78 | Loss: 1.1704259122474283 | Acc: 73.56 67628205128 \n",
      "epoch: 150\n",
      " batch_idx: 390 | Loss: 0.043314114336848564 | Acc: 98.744 717948718  test... ...\n",
      " batch_idx: 78 | Loss: 0.9724011100545714 | Acc: 77.19 44551282051 \n",
      "epoch: 151\n",
      " batch_idx: 390 | Loss: 0.04719917561925586 | Acc: 98.578 17948718 45 test... ...\n",
      " batch_idx: 78 | Loss: 0.972501286222965 | Acc: 77.84 455128205128 \n",
      "epoch: 152\n",
      " batch_idx: 390 | Loss: 0.04535325686149585 | Acc: 98.662 8782051282  test... ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_idx: 78 | Loss: 1.0315678010258493 | Acc: 77.32 1794871794  \n",
      "epoch: 153\n",
      " batch_idx: 390 | Loss: 0.0497405391014979 | Acc: 98.492 759615384616 test... ...\n",
      " batch_idx: 78 | Loss: 0.8965647773274893 | Acc: 79.09 50641025641 \n",
      "epoch: 154\n",
      " batch_idx: 390 | Loss: 0.04394725521030786 | Acc: 98.702 92307692308 test... ...\n",
      " batch_idx: 78 | Loss: 0.8022089996669866 | Acc: 81.2 91987179488  \n",
      "epoch: 155\n",
      " batch_idx: 390 | Loss: 0.0430303447072387 | Acc: 98.734 97435897436  test... ...\n",
      " batch_idx: 78 | Loss: 0.6901809784430492 | Acc: 83.33 33333333333 \n",
      "epoch: 156\n",
      " batch_idx: 390 | Loss: 0.05238144177838665 | Acc: 98.384 1025641026  test... ...\n",
      " batch_idx: 78 | Loss: 0.6480053003075756 | Acc: 84.15 63141025641 \n",
      "epoch: 157\n",
      " batch_idx: 390 | Loss: 0.04705542096834811 | Acc: 98.586 3717948718  test... ...\n",
      " batch_idx: 78 | Loss: 0.7304009553752367 | Acc: 81.73 73717948718 \n",
      "epoch: 158\n",
      " batch_idx: 390 | Loss: 0.047479442289799376 | Acc: 98.574 2756410257 test... ...\n",
      " batch_idx: 78 | Loss: 0.770183232388919 | Acc: 81.84 094551282051 \n",
      "epoch: 159\n",
      " batch_idx: 390 | Loss: 0.04994360182215186 | Acc: 98.458 53205128206 test... ...\n",
      " batch_idx: 78 | Loss: 0.9769165976138054 | Acc: 78.22 16025641026 \n",
      "epoch: 160\n",
      " batch_idx: 390 | Loss: 0.04445841861293291 | Acc: 98.692 705128206 5 test... ...\n",
      " batch_idx: 78 | Loss: 0.9064006333864187 | Acc: 78.4 44871794872  \n",
      "epoch: 161\n",
      " batch_idx: 390 | Loss: 0.04121114502248861 | Acc: 98.78 04807692308  test... ...\n",
      " batch_idx: 78 | Loss: 0.8456428201892708 | Acc: 81.04 66346153847 \n",
      "epoch: 162\n",
      " batch_idx: 390 | Loss: 0.04636545268737752 | Acc: 98.628 108974359 7 test... ...\n",
      " batch_idx: 78 | Loss: 1.4205477984645698 | Acc: 70.55 83653846153 \n",
      "epoch: 163\n",
      " batch_idx: 390 | Loss: 0.04109964419223006 | Acc: 98.782 4807692308  test... ...\n",
      " batch_idx: 78 | Loss: 0.6531795412302017 | Acc: 83.94 27884615384 \n",
      "epoch: 164\n",
      " batch_idx: 390 | Loss: 0.044821728854571155 | Acc: 98.62 79166666667 test... ...\n",
      " batch_idx: 78 | Loss: 1.00329218371005 | Acc: 77.24 235576923077  \n",
      "epoch: 165\n",
      " batch_idx: 390 | Loss: 0.043344861715841475 | Acc: 98.718 833333333  test... ...\n",
      " batch_idx: 78 | Loss: 0.8818030278139477 | Acc: 79.83 0833333333  \n",
      "epoch: 166\n",
      " batch_idx: 390 | Loss: 0.049419386417169094 | Acc: 98.496 576923077  test... ...\n",
      " batch_idx: 78 | Loss: 0.7302045675018166 | Acc: 83.66 8782051282  \n",
      "epoch: 167\n",
      " batch_idx: 390 | Loss: 0.050088677269494744 | Acc: 98.498 935897436  test... ...\n",
      " batch_idx: 78 | Loss: 0.7720226560589634 | Acc: 81.23 9358974359  \n",
      "epoch: 168\n",
      " batch_idx: 390 | Loss: 0.04955089128459506 | Acc: 98.548 7628205128  test... ...\n",
      " batch_idx: 78 | Loss: 0.9050255380099332 | Acc: 79.91 83653846153 \n",
      "epoch: 169\n",
      " batch_idx: 390 | Loss: 0.046256998067964676 | Acc: 98.61 7243589743  test... ...\n",
      " batch_idx: 78 | Loss: 0.8807374148429195 | Acc: 79.31 85897435898 \n",
      "epoch: 170\n",
      " batch_idx: 390 | Loss: 0.04827619080558 | Acc: 98.538 3966346153847  test... ...\n",
      " batch_idx: 78 | Loss: 0.9056660846064363 | Acc: 79.51 19551282051 \n",
      "epoch: 171\n",
      " batch_idx: 390 | Loss: 0.047026193879373235 | Acc: 98.556 91025641   test... ...\n",
      " batch_idx: 78 | Loss: 0.8639834545835664 | Acc: 80.14 22115384616 \n",
      "epoch: 172\n",
      " batch_idx: 390 | Loss: 0.047258173875377306 | Acc: 98.51 538461539   test... ...\n",
      " batch_idx: 78 | Loss: 1.088697106023378 | Acc: 75.8 8125 961039   \n",
      "epoch: 173\n",
      " batch_idx: 390 | Loss: 0.048195195005601627 | Acc: 98.576 076923077  test... ...\n",
      " batch_idx: 78 | Loss: 1.279845473132556 | Acc: 73.03 6875 909091  \n",
      "epoch: 174\n",
      " batch_idx: 390 | Loss: 0.040505569437733084 | Acc: 98.852 6346153847 test... ...\n",
      " batch_idx: 78 | Loss: 0.734669036125835 | Acc: 83.42 349358974359 \n",
      "epoch: 175\n",
      " batch_idx: 390 | Loss: 0.045204821117746326 | Acc: 98.64 82051282051 test... ...\n",
      " batch_idx: 78 | Loss: 0.9468955503234381 | Acc: 78.5 5625 5584415 \n",
      "epoch: 176\n",
      " batch_idx: 390 | Loss: 0.04684286744898314 | Acc: 98.594 467948718 8 test... ...\n",
      " batch_idx: 78 | Loss: 0.82070115472697 | Acc: 81.14 5985576923077 \n",
      "epoch: 177\n",
      " batch_idx: 390 | Loss: 0.04451654169498883 | Acc: 98.696 1025641026  test... ...\n",
      " batch_idx: 78 | Loss: 0.7990804540960095 | Acc: 81.16 83974358974 \n",
      "epoch: 178\n",
      " batch_idx: 390 | Loss: 0.0430676148551733 | Acc: 98.726 95833333333  test... ...\n",
      " batch_idx: 78 | Loss: 0.7493630606162397 | Acc: 82.37 79487179488 \n",
      "epoch: 179\n",
      " batch_idx: 390 | Loss: 0.04232517333315385 | Acc: 98.694 0705128206  test... ...\n",
      " batch_idx: 78 | Loss: 0.8214864696882949 | Acc: 81.37 16025641026 \n",
      "epoch: 180\n",
      " batch_idx: 390 | Loss: 0.050476800659885794 | Acc: 98.516  83547558  test... ...\n",
      " batch_idx: 78 | Loss: 0.6458731784096247 | Acc: 84.4 503205128206 \n",
      "epoch: 181\n",
      " batch_idx: 390 | Loss: 0.04115449364685342 | Acc: 98.812 9935897436  test... ...\n",
      " batch_idx: 78 | Loss: 1.2061881983204732 | Acc: 73.5 754807692308 \n",
      "epoch: 182\n",
      " batch_idx: 390 | Loss: 0.043201885817339046 | Acc: 98.696 1666666667 test... ...\n",
      " batch_idx: 78 | Loss: 0.8632410327090493 | Acc: 79.83 75641025641 \n",
      "epoch: 183\n",
      " batch_idx: 390 | Loss: 0.04942421565103866 | Acc: 98.51 625 25964 4  test... ...\n",
      " batch_idx: 78 | Loss: 1.046617034869858 | Acc: 77.48 9423076923   \n",
      "epoch: 184\n",
      " batch_idx: 390 | Loss: 0.042091990809154024 | Acc: 98.714 1025641 42 test... ...\n",
      " batch_idx: 78 | Loss: 1.1699414645569235 | Acc: 75.07 1282051282  \n",
      "epoch: 185\n",
      " batch_idx: 390 | Loss: 0.04827800575558029 | Acc: 98.538 6025641026  test... ...\n",
      " batch_idx: 78 | Loss: 1.0904267845274527 | Acc: 75.88 45833333333 \n",
      "epoch: 186\n",
      " batch_idx: 390 | Loss: 0.04925301677221075 | Acc: 98.568 2115384616  test... ...\n",
      " batch_idx: 78 | Loss: 1.0713828907737248 | Acc: 76.78 6858974359  \n",
      "epoch: 187\n",
      " batch_idx: 390 | Loss: 0.04541515352209206 | Acc: 98.68 88782051282  test... ...\n",
      " batch_idx: 78 | Loss: 1.250882383388809 | Acc: 74.04 847756410257 \n",
      "epoch: 188\n",
      " batch_idx: 390 | Loss: 0.04887908649490313 | Acc: 98.548 67628205128 test... ...\n",
      " batch_idx: 78 | Loss: 0.7899370732941206 | Acc: 81.42 25641025641 \n",
      "epoch: 189\n",
      " batch_idx: 390 | Loss: 0.04370790256468384 | Acc: 98.696 1346153847  test... ...\n",
      " batch_idx: 78 | Loss: 0.9118099880369404 | Acc: 79.5 719551282051 \n",
      "epoch: 190\n",
      " batch_idx: 390 | Loss: 0.047259513626966024 | Acc: 98.62 8846153847  test... ...\n",
      " batch_idx: 78 | Loss: 0.972818945782094 | Acc: 78.37 540064102564 \n",
      "epoch: 191\n",
      " batch_idx: 390 | Loss: 0.04760540248659413 | Acc: 98.58 3076923077   test... ...\n",
      " batch_idx: 78 | Loss: 0.8828185095062738 | Acc: 78.99 39423076923 \n",
      "epoch: 192\n",
      " batch_idx: 390 | Loss: 0.04744624910528398 | Acc: 98.6 9975961538461 test... ...\n",
      " batch_idx: 78 | Loss: 0.8432369865948641 | Acc: 80.82 32692307692 \n",
      "epoch: 193\n",
      " batch_idx: 390 | Loss: 0.044370232340510546 | Acc: 98.706 948717949  test... ...\n",
      " batch_idx: 78 | Loss: 0.8883810933632187 | Acc: 79.56 25961538461 \n",
      "epoch: 194\n",
      " batch_idx: 390 | Loss: 0.04961405669236579 | Acc: 98.514 217948718   test... ...\n",
      " batch_idx: 78 | Loss: 1.0172142401526245 | Acc: 77.51 02243589743 \n",
      "epoch: 195\n",
      " batch_idx: 390 | Loss: 0.04728502041810309 | Acc: 98.57 70833333333  test... ...\n",
      " batch_idx: 78 | Loss: 1.057318995270548 | Acc: 76.54 246794871794 \n",
      "epoch: 196\n",
      " batch_idx: 390 | Loss: 0.045719465726743576 | Acc: 98.664 538461539  test... ...\n",
      " batch_idx: 78 | Loss: 0.8685205312846582 | Acc: 79.66 0384615384  \n",
      "epoch: 197\n",
      " batch_idx: 390 | Loss: 0.04738976759716983 | Acc: 98.532 705128206   test... ...\n",
      " batch_idx: 78 | Loss: 1.0099098735217806 | Acc: 77.45 89423076923 \n",
      "epoch: 198\n",
      " batch_idx: 390 | Loss: 0.04571321689526139 | Acc: 98.648 3653846153  test... ...\n",
      " batch_idx: 78 | Loss: 0.7314105735549444 | Acc: 82.38 77884615384 \n",
      "epoch: 199\n",
      " batch_idx: 390 | Loss: 0.04822185590787007 | Acc: 98.55 36858974359  test... ...\n",
      " batch_idx: 78 | Loss: 0.8119430017622211 | Acc: 80.97 55128205128 "
     ]
    }
   ],
   "source": [
    "# 对抗训练\n",
    "epsilon = 0.3\n",
    "\n",
    "for epoch in range(200):\n",
    "    model_adv.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print('\\nepoch: {}'.format(epoch))\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        inputs, sign = my_fgsm(inputs, targets, model_adv, criterion, epsilon, DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_adv(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        print('\\r batch_idx: {} | Loss: {} | Acc: {} '.format(batch_idx, \n",
    "                                                              train_loss/(batch_idx+1), 100.*correct/total ), end='')\n",
    "    print('test... ...')\n",
    "\n",
    "    model_adv.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        inputs, sign = my_fgsm(inputs, targets, model_adv, criterion, epsilon, DEVICE)\n",
    "        outputs = model_adv(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        print('\\r batch_idx: {} | Loss: {} | Acc: {} '.format(batch_idx, \n",
    "                                                          test_loss/(batch_idx+1), 100.*correct/total ), end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 保存模型\n",
    "model_adv = model_adv.cpu()\n",
    "if not os.path.exists('./model'):\n",
    "    os.makedirs('./model')\n",
    "if NORMALIZE:\n",
    "    model_path = './model/ResNet18_CIFAR10_adv.pt'\n",
    "else:\n",
    "    model_path = './model/ResNet18_CIFAR10_unNormalize_adv.pt'\n",
    "torch.save(model_adv.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model: ./model/ResNet18_CIFAR10_adv.pt\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# 读取对抗训练模型\n",
    "# 构建网络结构\n",
    "if NORMALIZE:\n",
    "    model_path = './model/ResNet18_CIFAR10_adv.pt'\n",
    "\n",
    "\n",
    "model_adv = ResNet18()\n",
    "model_adv = model_adv.to(DEVICE)\n",
    "model_adv = torch.nn.DataParallel(model_adv, device_ids=[1,2,3])\n",
    "# , map_location=lambda storage, loc: storage.cuda(1)\n",
    "print('load model: {}'.format(model_path))\n",
    "model_adv.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# model_adv = model_adv.cpu()\n",
    "# model_adv = model_adv.to(DEVICE)\n",
    "# model_adv = torch.nn.DataParallel(model_adv)\n",
    "\n",
    "# # model_adv = torch.nn.DataParallel(model_adv)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_adv.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adversarial test... ...\n",
      " batch_idx: 78 | Loss: 0.8189108413231524 | Acc: 80.97 58333333333 "
     ]
    }
   ],
   "source": [
    "# 对砍测试\n",
    "print('adversarial test... ...')\n",
    "\n",
    "epsilon = 0.3\n",
    "model_adv.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "    inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "    inputs, sign = my_fgsm(inputs, targets, model_adv, criterion, epsilon, DEVICE)\n",
    "    outputs = model_adv(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    test_loss += loss.item()\n",
    "    _, predicted = outputs.max(1)\n",
    "    total += targets.size(0)\n",
    "    correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    print('\\r batch_idx: {} | Loss: {} | Acc: {} '.format(batch_idx, \n",
    "                                                      test_loss/(batch_idx+1), 100.*correct/total ), end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "185px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
