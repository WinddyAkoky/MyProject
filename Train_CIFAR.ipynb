{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# winddy\n",
    "\n",
    "用训练集CIFAR 对抗训练 ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torchvision.transforms as transforms \n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from LeNet import LeNet\n",
    "import sys\n",
    "sys.path.append('./model')\n",
    "from resnet import ResNet18\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NORMALIZE = True\n",
    "RESUME = True\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str,[0,2,7]))\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if NORMALIZE:\n",
    "    trans_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    trans_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "else:\n",
    "    trans_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    trans_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_home = '/data/winddy/'\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root=os.path.join(data_home, 'dataset/CIFAR10'), train=True, download=True, transform=trans_train)\n",
    "test_set = torchvision.datasets.CIFAR10(root=os.path.join(data_home, 'dataset/CIFAR10'), train=False, download=True, transform=trans_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resuming from checkpoint..\n"
     ]
    }
   ],
   "source": [
    "# 构建网络结构\n",
    "net = ResNet18()\n",
    "net = net.to(DEVICE)\n",
    "net = torch.nn.DataParallel(net)\n",
    "\n",
    "if RESUME:\n",
    "# Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        print('\\r batch_idx: {} | Loss: {} | Acc: {} '.format(batch_idx, \n",
    "                                                              train_loss/(batch_idx+1), 100.*correct/total ), end='')\n",
    "#         progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "#             % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            print('\\r batch_idx: {} | Loss: {} | Acc: {} '.format(batch_idx, \n",
    "                                                              test_loss/(batch_idx+1), 100.*correct/total ), end='')\n",
    "#             progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "#                 % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "            \n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.pth')\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      " batch_idx: 78 | Loss: 0.3216810831917992 | Acc: 91.05 565705128206   Saving..\n",
      "\n",
      "Epoch: 1\n",
      " batch_idx: 78 | Loss: 0.32489821475140657 | Acc: 91.45 36217948718   Saving..\n",
      "\n",
      "Epoch: 2\n",
      " batch_idx: 78 | Loss: 0.30908908081960074 | Acc: 91.36 15384615384   \n",
      "Epoch: 3\n",
      " batch_idx: 78 | Loss: 0.30799488081962245 | Acc: 91.72 6282051282  2 Saving..\n",
      "\n",
      "Epoch: 4\n",
      " batch_idx: 78 | Loss: 0.2805704056179222 | Acc: 92.44 790064102564   Saving..\n",
      "\n",
      "Epoch: 5\n",
      " batch_idx: 78 | Loss: 0.34105642347396176 | Acc: 90.79 5641025641    \n",
      "Epoch: 6\n",
      " batch_idx: 78 | Loss: 0.33029308311546907 | Acc: 91.59 5448717949    \n",
      "Epoch: 7\n",
      " batch_idx: 78 | Loss: 0.306318882051148 | Acc: 92.28 767628205128  6 \n",
      "Epoch: 8\n",
      " batch_idx: 78 | Loss: 0.33230514880977097 | Acc: 90.96 5608974359    \n",
      "Epoch: 9\n",
      " batch_idx: 78 | Loss: 0.3095919775623309 | Acc: 91.76 79487179488    \n",
      "Epoch: 10\n",
      " batch_idx: 78 | Loss: 0.29556303084651125 | Acc: 91.6 53846153847    \n",
      "Epoch: 11\n",
      " batch_idx: 78 | Loss: 0.31615748718569553 | Acc: 91.16 83333333333   \n",
      "Epoch: 12\n",
      " batch_idx: 78 | Loss: 0.2934104184372516 | Acc: 92.07 729166666667   \n",
      "Epoch: 13\n",
      " batch_idx: 78 | Loss: 0.32414054399049735 | Acc: 91.19 1346153847    \n",
      "Epoch: 14\n",
      " batch_idx: 78 | Loss: 0.31616758875831774 | Acc: 91.33 15384615384   \n",
      "Epoch: 15\n",
      " batch_idx: 78 | Loss: 0.28658312832630134 | Acc: 92.24 9615384616    \n",
      "Epoch: 16\n",
      " batch_idx: 78 | Loss: 0.319260482923894 | Acc: 91.27 602564102564  7 \n",
      "Epoch: 17\n",
      " batch_idx: 78 | Loss: 0.2942156106988086 | Acc: 91.97 16346153847    \n",
      "Epoch: 18\n",
      " batch_idx: 78 | Loss: 0.33491779184794124 | Acc: 90.92 8076923077  2 \n",
      "Epoch: 19\n",
      " batch_idx: 78 | Loss: 0.33408242407478866 | Acc: 90.96 608974359 4 6 \n",
      "Epoch: 20\n",
      " batch_idx: 78 | Loss: 0.3192706841833984 | Acc: 90.96 54487179488    \n",
      "Epoch: 21\n",
      " batch_idx: 78 | Loss: 0.32537268536000313 | Acc: 91.17 8141025641  2 \n",
      "Epoch: 22\n",
      " batch_idx: 78 | Loss: 0.2962404461223868 | Acc: 91.85 697115384616 7 \n",
      "Epoch: 23\n",
      " batch_idx: 78 | Loss: 0.3195494137987306 | Acc: 91.49 3782051282     \n",
      "Epoch: 24\n",
      " batch_idx: 78 | Loss: 0.3254856331438958 | Acc: 91.29 04166666667    \n",
      "Epoch: 25\n",
      " batch_idx: 78 | Loss: 0.3175015026066877 | Acc: 91.46 63141025641    \n",
      "Epoch: 26\n",
      " batch_idx: 78 | Loss: 0.2790357185692727 | Acc: 92.01 721153846153   \n",
      "Epoch: 27\n",
      " batch_idx: 78 | Loss: 0.2918363036234168 | Acc: 92.02 21153846153  1 \n",
      "Epoch: 28\n",
      " batch_idx: 78 | Loss: 0.30932748742118665 | Acc: 91.51 42628205128   \n",
      "Epoch: 29\n",
      " batch_idx: 78 | Loss: 0.28311156132553195 | Acc: 92.3 67628205128    \n",
      "Epoch: 30\n",
      " batch_idx: 78 | Loss: 0.29786745949259286 | Acc: 91.98 6346153847    \n",
      "Epoch: 31\n",
      " batch_idx: 78 | Loss: 0.3048351978576636 | Acc: 92.01 24358974359  9 \n",
      "Epoch: 32\n",
      " batch_idx: 78 | Loss: 0.3280221072933342 | Acc: 90.94 548076923077   \n",
      "Epoch: 33\n",
      " batch_idx: 78 | Loss: 0.2936062047470219 | Acc: 91.92 708333333333   \n",
      "Epoch: 34\n",
      " batch_idx: 78 | Loss: 0.31964272241803665 | Acc: 91.89 1923076923    \n",
      "Epoch: 35\n",
      " batch_idx: 78 | Loss: 0.30083028227090836 | Acc: 91.59 2243589743    \n",
      "Epoch: 36\n",
      " batch_idx: 78 | Loss: 0.33183301716477054 | Acc: 91.08 3717948718    \n",
      "Epoch: 37\n",
      " batch_idx: 78 | Loss: 0.2944538602723351 | Acc: 91.85 9391025641 5 3 \n",
      "Epoch: 38\n",
      " batch_idx: 78 | Loss: 0.3029754646594011 | Acc: 91.57 649038461539   \n",
      "Epoch: 39\n",
      " batch_idx: 78 | Loss: 0.30429465276531026 | Acc: 91.81 89102564102   \n",
      "Epoch: 40\n",
      " batch_idx: 78 | Loss: 0.31325579538375514 | Acc: 91.25 7756410257  7 \n",
      "Epoch: 41\n",
      " batch_idx: 78 | Loss: 0.29347298982777176 | Acc: 92.31 923076923 6   \n",
      "Epoch: 42\n",
      " batch_idx: 78 | Loss: 0.3070289201185673 | Acc: 91.83 95512820512    \n",
      "Epoch: 43\n",
      " batch_idx: 78 | Loss: 0.2810292900363101 | Acc: 92.22 54807692308    \n",
      "Epoch: 44\n",
      " batch_idx: 78 | Loss: 0.27642204433302336 | Acc: 92.17 45192307692   \n",
      "Epoch: 45\n",
      " batch_idx: 78 | Loss: 0.31934192937008943 | Acc: 91.26 2564102564    \n",
      "Epoch: 46\n",
      " batch_idx: 78 | Loss: 0.29699492459244364 | Acc: 91.84 9391025641  7 \n",
      "Epoch: 47\n",
      " batch_idx: 78 | Loss: 0.3070577937590925 | Acc: 91.73 67467948718    \n",
      "Epoch: 48\n",
      " batch_idx: 78 | Loss: 0.28963940713224534 | Acc: 91.95 11538461539 1 \n",
      "Epoch: 49\n",
      " batch_idx: 78 | Loss: 0.30142221137692654 | Acc: 91.48 4615384616  7 \n",
      "Epoch: 50\n",
      " batch_idx: 78 | Loss: 0.27568015390181844 | Acc: 92.38 2051282051    \n",
      "Epoch: 51\n",
      " batch_idx: 78 | Loss: 0.30230296110805077 | Acc: 91.59 3846153847    \n",
      "Epoch: 52\n",
      " batch_idx: 78 | Loss: 0.2752326858949058 | Acc: 92.43 88461538461    \n",
      "Epoch: 53\n",
      " batch_idx: 78 | Loss: 0.3214120546096488 | Acc: 91.54 645833333333   \n",
      "Epoch: 54\n",
      " batch_idx: 78 | Loss: 0.35513981969296177 | Acc: 90.35 5128205128  7 \n",
      "Epoch: 55\n",
      " batch_idx: 78 | Loss: 0.30524442750441877 | Acc: 91.48 4615384616    \n",
      "Epoch: 56\n",
      " batch_idx: 78 | Loss: 0.296368096353887 | Acc: 91.72 676282051282    \n",
      "Epoch: 57\n",
      " batch_idx: 78 | Loss: 0.3006905451228347 | Acc: 91.64 63461538461    \n",
      "Epoch: 58\n",
      " batch_idx: 78 | Loss: 0.29411992987122715 | Acc: 91.84 391025641 5   \n",
      "Epoch: 59\n",
      " batch_idx: 78 | Loss: 0.2909096234206912 | Acc: 92.31 67628205128  8 \n",
      "Epoch: 60\n",
      " batch_idx: 78 | Loss: 0.3006443629347825 | Acc: 91.92 0673076923 9   \n",
      "Epoch: 61\n",
      " batch_idx: 78 | Loss: 0.2855178958064393 | Acc: 92.22 54807692308    \n",
      "Epoch: 62\n",
      " batch_idx: 78 | Loss: 0.3031504840790471 | Acc: 91.43 628205128206   \n",
      "Epoch: 63\n",
      " batch_idx: 78 | Loss: 0.28759338301193865 | Acc: 92.02 5961538461  7 \n",
      "Epoch: 64\n",
      " batch_idx: 78 | Loss: 0.3252347913346713 | Acc: 91.16 584935897436 3 \n",
      "Epoch: 65\n",
      " batch_idx: 78 | Loss: 0.3007581562652618 | Acc: 91.74 487179488 3    \n",
      "Epoch: 66\n",
      " batch_idx: 78 | Loss: 0.30799739025061645 | Acc: 91.4 623397435898   \n",
      "Epoch: 67\n",
      " batch_idx: 78 | Loss: 0.3233033284733567 | Acc: 91.55 649038461539   \n",
      "Epoch: 68\n",
      " batch_idx: 78 | Loss: 0.2843238088903548 | Acc: 91.85 697115384616   \n",
      "Epoch: 69\n",
      " batch_idx: 78 | Loss: 0.33064417012884645 | Acc: 91.02 64102564102 1 \n",
      "Epoch: 70\n",
      " batch_idx: 78 | Loss: 0.3065509292521054 | Acc: 91.68 66826923077  9 \n",
      "Epoch: 71\n",
      " batch_idx: 78 | Loss: 0.2885305554240565 | Acc: 92.23 58012820512    \n",
      "Epoch: 72\n",
      " batch_idx: 78 | Loss: 0.3122715815147267 | Acc: 91.28 2564102564 7   \n",
      "Epoch: 73\n",
      " batch_idx: 78 | Loss: 0.28362257165622107 | Acc: 92.19 5 168831169 6 \n",
      "Epoch: 74\n",
      " batch_idx: 78 | Loss: 0.29895930642945856 | Acc: 91.84 97115384616 2 \n",
      "Epoch: 75\n",
      " batch_idx: 78 | Loss: 0.31422380065616173 | Acc: 91.77 108974359 3   \n",
      "Epoch: 76\n",
      " batch_idx: 78 | Loss: 0.30777120477036585 | Acc: 91.98 19551282051 7 \n",
      "Epoch: 77\n",
      " batch_idx: 78 | Loss: 0.32299065684215933 | Acc: 91.47 3012820512    \n",
      "Epoch: 78\n",
      " batch_idx: 78 | Loss: 0.3116144936861871 | Acc: 91.56 47435897436    \n",
      "Epoch: 79\n",
      " batch_idx: 78 | Loss: 0.29128114652784565 | Acc: 91.88 01923076923 2 \n",
      "Epoch: 80\n",
      " batch_idx: 78 | Loss: 0.31853212765123273 | Acc: 91.14 173076923     \n",
      "Epoch: 81\n",
      " batch_idx: 78 | Loss: 0.2947632379924195 | Acc: 92.01 719551282051   \n",
      "Epoch: 82\n",
      " batch_idx: 78 | Loss: 0.2935314078496981 | Acc: 91.93 708333333333   \n",
      "Epoch: 83\n",
      " batch_idx: 78 | Loss: 0.2840912592939184 | Acc: 92.41 3653846153 7   \n",
      "Epoch: 84\n",
      " batch_idx: 78 | Loss: 0.26749689037664026 | Acc: 92.64 20512820512 9 Saving..\n",
      "\n",
      "Epoch: 85\n",
      " batch_idx: 78 | Loss: 0.32521463157255437 | Acc: 91.15 173076923 1 3 \n",
      "Epoch: 86\n",
      " batch_idx: 78 | Loss: 0.28429368949389155 | Acc: 92.29 4423076923  1 \n",
      "Epoch: 87\n",
      " batch_idx: 78 | Loss: 0.3299524484178688 | Acc: 90.7 509615384616    \n",
      "Epoch: 88\n",
      " batch_idx: 78 | Loss: 0.26401367887288707 | Acc: 93.01 141025641   2 Saving..\n",
      "\n",
      "Epoch: 89\n",
      " batch_idx: 78 | Loss: 0.30369597220722633 | Acc: 91.88 01923076923   \n",
      "Epoch: 90\n",
      " batch_idx: 78 | Loss: 0.2754922584642338 | Acc: 92.8 849358974359    \n",
      "Epoch: 91\n",
      " batch_idx: 78 | Loss: 0.3134756922910485 | Acc: 91.62 657051282051   \n",
      "Epoch: 92\n",
      " batch_idx: 78 | Loss: 0.3346938711555698 | Acc: 90.94 4967948718 2   \n",
      "Epoch: 93\n",
      " batch_idx: 78 | Loss: 0.2829639564586591 | Acc: 92.17 745192307692   \n",
      "Epoch: 94\n",
      " batch_idx: 78 | Loss: 0.31110556140730655 | Acc: 91.55 49038461539   \n",
      "Epoch: 95\n",
      " batch_idx: 78 | Loss: 0.30635013836848585 | Acc: 92.16 5192307692    \n",
      "Epoch: 96\n",
      " batch_idx: 78 | Loss: 0.29478397380702104 | Acc: 91.78 5897435898    \n",
      "Epoch: 97\n",
      " batch_idx: 78 | Loss: 0.30408880312608766 | Acc: 91.95 09935897436   \n",
      "Epoch: 98\n",
      " batch_idx: 78 | Loss: 0.2992080223711231 | Acc: 91.52 41025641026  7 \n",
      "Epoch: 99\n",
      " batch_idx: 78 | Loss: 0.27791364717332623 | Acc: 92.58 5 805194805 9 \n",
      "Epoch: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_idx: 78 | Loss: 0.35714713550066646 | Acc: 90.82 0448717949  7 \n",
      "Epoch: 101\n",
      " batch_idx: 78 | Loss: 0.3014474937432929 | Acc: 91.73 76282051282    \n",
      "Epoch: 102\n",
      " batch_idx: 78 | Loss: 0.28286192122894 | Acc: 92.5 4979967948718 4   \n",
      "Epoch: 103\n",
      " batch_idx: 78 | Loss: 0.30559930441123023 | Acc: 91.76 8108974359    \n",
      "Epoch: 104\n",
      " batch_idx: 78 | Loss: 0.31470954908600335 | Acc: 91.83 2307692308  6 \n",
      "Epoch: 105\n",
      " batch_idx: 78 | Loss: 0.29612850935398777 | Acc: 91.94 9935897436  5 \n",
      "Epoch: 106\n",
      " batch_idx: 78 | Loss: 0.33725800299191777 | Acc: 91.36 15384615384   \n",
      "Epoch: 107\n",
      " batch_idx: 78 | Loss: 0.29265336173621914 | Acc: 92.02 2756410257  9 \n",
      "Epoch: 108\n",
      " batch_idx: 78 | Loss: 0.29337241962740696 | Acc: 91.58 0641025641  1 \n",
      "Epoch: 109\n",
      " batch_idx: 78 | Loss: 0.31668127243277394 | Acc: 91.69 73076923077   \n",
      "Epoch: 110\n",
      " batch_idx: 78 | Loss: 0.31059765711992604 | Acc: 91.44 29807692308   \n",
      "Epoch: 111\n",
      " batch_idx: 78 | Loss: 0.2924452227882192 | Acc: 92.04 25961538461    \n",
      "Epoch: 112\n",
      " batch_idx: 78 | Loss: 0.2902297028635122 | Acc: 91.85 69391025641  8 \n",
      "Epoch: 113\n",
      " batch_idx: 78 | Loss: 0.31031425297260284 | Acc: 92.13 40384615384 3 \n",
      "Epoch: 114\n",
      " batch_idx: 78 | Loss: 0.28964699578436115 | Acc: 92.05 29166666667   \n",
      "Epoch: 115\n",
      " batch_idx: 78 | Loss: 0.34695788369148595 | Acc: 90.86 35256410257   \n",
      "Epoch: 116\n",
      " batch_idx: 78 | Loss: 0.33800377574148055 | Acc: 91.21 2948717949    \n",
      "Epoch: 117\n",
      " batch_idx: 78 | Loss: 0.3209572484410262 | Acc: 91.46 634615384616 2 \n",
      "Epoch: 118\n",
      " batch_idx: 78 | Loss: 0.33819044749193555 | Acc: 90.74 7628205128    \n",
      "Epoch: 119\n",
      " batch_idx: 78 | Loss: 0.2925804030574575 | Acc: 91.84 92307692308  9 \n",
      "Epoch: 120\n",
      " batch_idx: 78 | Loss: 0.2920967186742191 | Acc: 91.86 97115384616    \n",
      "Epoch: 121\n",
      " batch_idx: 78 | Loss: 0.3424856019548223 | Acc: 91.09 78525641026    \n",
      "Epoch: 122\n",
      " batch_idx: 78 | Loss: 0.3109511789641803 | Acc: 91.59 655448717949   \n",
      "Epoch: 123\n",
      " batch_idx: 78 | Loss: 0.31970687550079974 | Acc: 91.33 858974359 2   \n",
      "Epoch: 124\n",
      " batch_idx: 78 | Loss: 0.352425411626508 | Acc: 91.05 572115384616  5 \n",
      "Epoch: 125\n",
      " batch_idx: 78 | Loss: 0.3128921540477608 | Acc: 91.49 636217948718   \n",
      "Epoch: 126\n",
      " batch_idx: 78 | Loss: 0.3044353063347973 | Acc: 91.59 57051282051    \n",
      "Epoch: 127\n",
      " batch_idx: 78 | Loss: 0.3201814102409761 | Acc: 91.53 4423076923     \n",
      "Epoch: 128\n",
      " batch_idx: 78 | Loss: 0.31790476660185224 | Acc: 91.48 36217948718 4 \n",
      "Epoch: 129\n",
      " batch_idx: 78 | Loss: 0.3242485581696788 | Acc: 91.07 570512820512   \n",
      "Epoch: 130\n",
      " batch_idx: 78 | Loss: 0.2845466978187802 | Acc: 92.26 761217948718 7 \n",
      "Epoch: 131\n",
      " batch_idx: 78 | Loss: 0.2950056321636031 | Acc: 92.13 40384615384    \n",
      "Epoch: 132\n",
      " batch_idx: 78 | Loss: 0.2967997483814819 | Acc: 91.6 8653846153847   \n",
      "Epoch: 133\n",
      " batch_idx: 78 | Loss: 0.29986268357385565 | Acc: 92.13 1987179488    \n",
      "Epoch: 134\n",
      " batch_idx: 78 | Loss: 0.30474026670938803 | Acc: 91.74 7884615384    \n",
      "Epoch: 135\n",
      " batch_idx: 78 | Loss: 0.31583481404600267 | Acc: 91.43 9807692308    \n",
      "Epoch: 136\n",
      " batch_idx: 78 | Loss: 0.2888034867900836 | Acc: 92.51 02884615384    \n",
      "Epoch: 137\n",
      " batch_idx: 78 | Loss: 0.3268611591450776 | Acc: 91.46 634615384616   \n",
      "Epoch: 138\n",
      " batch_idx: 78 | Loss: 0.2723201958910574 | Acc: 92.63 820512820512 3 \n",
      "Epoch: 139\n",
      " batch_idx: 78 | Loss: 0.2833182923024214 | Acc: 92.63 23717948718    \n",
      "Epoch: 140\n",
      " batch_idx: 78 | Loss: 0.28258879709092877 | Acc: 92.45 1666666667    \n",
      "Epoch: 141\n",
      " batch_idx: 78 | Loss: 0.25782923447557643 | Acc: 92.95 71794871794 1 \n",
      "Epoch: 142\n",
      " batch_idx: 78 | Loss: 0.297066450401952 | Acc: 92.13 3741987179488   \n",
      "Epoch: 143\n",
      " batch_idx: 78 | Loss: 0.2943119978225684 | Acc: 92.09 33974358974    \n",
      "Epoch: 144\n",
      " batch_idx: 78 | Loss: 0.26422781591551214 | Acc: 92.92 6987179488    \n",
      "Epoch: 145\n",
      " batch_idx: 78 | Loss: 0.32724736951574496 | Acc: 91.7 69871794872  3 \n",
      "Epoch: 146\n",
      " batch_idx: 78 | Loss: 0.30166767122624794 | Acc: 91.75 9487179488    \n",
      "Epoch: 147\n",
      " batch_idx: 78 | Loss: 0.30053608685354644 | Acc: 91.98 6346153847    \n",
      "Epoch: 148\n",
      " batch_idx: 78 | Loss: 0.28090071668730504 | Acc: 92.45 26923077      \n",
      "Epoch: 149\n",
      " batch_idx: 78 | Loss: 0.28937567317787605 | Acc: 92.54 04487179488   \n",
      "Epoch: 150\n",
      " batch_idx: 78 | Loss: 0.2934352036895631 | Acc: 91.9 705128205128    \n",
      "Epoch: 151\n",
      " batch_idx: 78 | Loss: 0.2849081343487848 | Acc: 92.32 923076923 14   \n",
      "Epoch: 152\n",
      " batch_idx: 78 | Loss: 0.29882042138259624 | Acc: 91.41 23397435898   \n",
      "Epoch: 153\n",
      " batch_idx: 78 | Loss: 0.3544560472327697 | Acc: 90.88 41666666667    \n",
      "Epoch: 154\n",
      " batch_idx: 78 | Loss: 0.2728568749337257 | Acc: 92.97 875 72727273   \n",
      "Epoch: 155\n",
      " batch_idx: 78 | Loss: 0.2958505424093219 | Acc: 92.21 54807692308    \n",
      "Epoch: 156\n",
      " batch_idx: 78 | Loss: 0.27886726037610937 | Acc: 92.61 1891025641  3 \n",
      "Epoch: 157\n",
      " batch_idx: 78 | Loss: 0.27604708914892584 | Acc: 92.6 1891025641 7 1 \n",
      "Epoch: 158\n",
      " batch_idx: 78 | Loss: 0.2790948041254961 | Acc: 92.16 4358974359 7 8 \n",
      "Epoch: 159\n",
      " batch_idx: 78 | Loss: 0.28370771236434766 | Acc: 92.06 29166666667 2 \n",
      "Epoch: 160\n",
      " batch_idx: 78 | Loss: 0.34979256072753595 | Acc: 90.82 5256410257    \n",
      "Epoch: 161\n",
      " batch_idx: 78 | Loss: 0.29613978510038763 | Acc: 92.15 45192307692   \n",
      "Epoch: 162\n",
      " batch_idx: 78 | Loss: 0.28940038439593735 | Acc: 92.45 8076923077    \n",
      "Epoch: 163\n",
      " batch_idx: 78 | Loss: 0.3015337046000022 | Acc: 91.36 61858974359    \n",
      "Epoch: 164\n",
      " batch_idx: 78 | Loss: 0.3271047903012626 | Acc: 91.49 41025641026    \n",
      "Epoch: 165\n",
      " batch_idx: 78 | Loss: 0.2987546289834795 | Acc: 92.25 76282051282    \n",
      "Epoch: 166\n",
      " batch_idx: 78 | Loss: 0.2934165625255319 | Acc: 92.18 48397435898    \n",
      "Epoch: 167\n",
      " batch_idx: 78 | Loss: 0.3010344306313539 | Acc: 91.78 84294871794  7 \n",
      "Epoch: 168\n",
      " batch_idx: 78 | Loss: 0.3228893239475504 | Acc: 91.64 63461538461    \n",
      "Epoch: 169\n",
      " batch_idx: 78 | Loss: 0.3744085291518441 | Acc: 90.87 41666666667    \n",
      "Epoch: 170\n",
      " batch_idx: 78 | Loss: 0.28741672437025023 | Acc: 92.08 2371794872  2 \n",
      "Epoch: 171\n",
      " batch_idx: 78 | Loss: 0.2774922315832935 | Acc: 92.25 58012820512    \n",
      "Epoch: 172\n",
      " batch_idx: 78 | Loss: 0.3081041668789296 | Acc: 91.47 634615384616 3 \n",
      "Epoch: 173\n",
      " batch_idx: 78 | Loss: 0.2901058008399191 | Acc: 92.29 767628205128 6 \n",
      "Epoch: 174\n",
      " batch_idx: 78 | Loss: 0.28272517643208744 | Acc: 92.32 923076923     \n",
      "Epoch: 175\n",
      " batch_idx: 78 | Loss: 0.28906266032894956 | Acc: 92.33 70833333333   \n",
      "Epoch: 176\n",
      " batch_idx: 78 | Loss: 0.3371319111578072 | Acc: 91.47 636217948718   \n",
      "Epoch: 177\n",
      " batch_idx: 78 | Loss: 0.3043360149935831 | Acc: 91.66 6826923077     \n",
      "Epoch: 178\n",
      " batch_idx: 78 | Loss: 0.3023805929512917 | Acc: 92.34 75641025641  9 \n",
      "Epoch: 179\n",
      " batch_idx: 78 | Loss: 0.2722416131085233 | Acc: 92.57 125 2857143  9 \n",
      "Epoch: 180\n",
      " batch_idx: 78 | Loss: 0.27772503034977974 | Acc: 92.27 6282051282    \n",
      "Epoch: 181\n",
      " batch_idx: 78 | Loss: 0.31212265082175217 | Acc: 91.85 115384616     \n",
      "Epoch: 182\n",
      " batch_idx: 78 | Loss: 0.2775783830239803 | Acc: 92.59 814102564102   \n",
      "Epoch: 183\n",
      " batch_idx: 78 | Loss: 0.3010437324454513 | Acc: 92.4 9783653846153   \n",
      "Epoch: 184\n",
      " batch_idx: 78 | Loss: 0.26517994532102274 | Acc: 92.84 2564102564    \n",
      "Epoch: 185\n",
      " batch_idx: 78 | Loss: 0.26668466250353223 | Acc: 92.91 63782051282 1 \n",
      "Epoch: 186\n",
      " batch_idx: 78 | Loss: 0.27979057725471784 | Acc: 92.38 78846153847   \n",
      "Epoch: 187\n",
      " batch_idx: 78 | Loss: 0.3039964147006409 | Acc: 91.88 698717948718 4 \n",
      "Epoch: 188\n",
      " batch_idx: 78 | Loss: 0.28612308732316466 | Acc: 92.46 326923077   3 \n",
      "Epoch: 189\n",
      " batch_idx: 78 | Loss: 0.33133180533783346 | Acc: 91.28 07371794872   \n",
      "Epoch: 190\n",
      " batch_idx: 78 | Loss: 0.29320555161448975 | Acc: 91.97 13141025641   \n",
      "Epoch: 191\n",
      " batch_idx: 78 | Loss: 0.28820019375674333 | Acc: 92.17 45192307692   \n",
      "Epoch: 192\n",
      " batch_idx: 78 | Loss: 0.29826160906990873 | Acc: 91.71 7467948718    \n",
      "Epoch: 193\n",
      " batch_idx: 78 | Loss: 0.2891857273216489 | Acc: 91.94 09935897436    \n",
      "Epoch: 194\n",
      " batch_idx: 78 | Loss: 0.28545036397968665 | Acc: 92.33 4038461539    \n",
      "Epoch: 195\n",
      " batch_idx: 78 | Loss: 0.34086059316804135 | Acc: 90.53 8717948718  8 \n",
      "Epoch: 196\n",
      " batch_idx: 78 | Loss: 0.286767717686635 | Acc: 92.21 0753205128206   \n",
      "Epoch: 197\n",
      " batch_idx: 78 | Loss: 0.2751038432875766 | Acc: 92.57 810897435898   \n",
      "Epoch: 198\n",
      " batch_idx: 78 | Loss: 0.26892710118731367 | Acc: 92.43 88461538461   \n",
      "Epoch: 199\n",
      " batch_idx: 78 | Loss: 0.2576070687250246 | Acc: 92.99 78205128206    "
     ]
    }
   ],
   "source": [
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0\n",
    "for epoch in range(start_epoch, start_epoch+200):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 保存模型\n",
    "if not os.path.exists('./model'):\n",
    "    os.makedirs('./model')\n",
    "if NORMALIZE:\n",
    "    model_path = './model/ResNet18_CIFAR10.pt'\n",
    "else:\n",
    "    model_path = './model/ResNet18_CIFAR10_unNormalize.pt'\n",
    "torch.save(net.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
